{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["3Xnx6cHSSa1E","yJbjmH2zuvKO"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"x9cduLOoSNQ5"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import sklearn\n","from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n","from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, classification_report\n","from keras.callbacks import EarlyStopping\n","import matplotlib.pylab as plt\n","import os\n","from keras.models import Sequential, Model\n","from keras.layers import Input, Dense, Dropout, Flatten, ConvLSTM2D, TimeDistributed, Bidirectional, LSTM\n","from keras.utils import Sequence\n","from tensorflow.keras.callbacks import Callback\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import tempfile\n","\n","\n","\n","class DataGen(Sequence):\n","    \"\"\" A sequence of data for training/test/validation, loaded from memory\n","    batch by batch. Extends the tensorflow.keras.utils.Sequence: https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n","\n","    Attributes\n","    ----------\n","    base_path : str\n","                path to the folder including the samples.\n","    filenames : list<str>\n","                list of sample filenames.\n","    labels : list<str>\n","             list of sample labels.\n","    batch_size : int\n","                 batch size to load samples\n","\n","    \"\"\"\n","\n","    def __init__(self, base_path, filenames, labels, batch_size, Preprocess_input):\n","        self.base_path = base_path\n","        self.filenames = filenames\n","        self.labels = labels\n","        self.batch_size = batch_size\n","        self.Preprocess_input = Preprocess_input\n","\n","    def __len__(self):\n","        return (np.ceil(len(self.filenames) / float(self.batch_size))).astype(int)\n","\n","    def __getitem__(self, idx):\n","        batch_x = self.filenames[idx * self.batch_size: (idx + 1) * self.batch_size]\n","        batch_y = self.labels[idx * self.batch_size: (idx + 1) * self.batch_size]\n","\n","        return np.array([self.Preprocess_input(np.load(os.path.join(self.base_path, file_name))) for file_name in batch_x]), np.array(batch_y)\n","\n","def GetPretrainedModel(ModelConstructor, input_shape=(224, 224, 3), print_summary=True):\n","    \"\"\" Builds the MobileNet V3 Small 2D with the Imagenet weights, freezing all layers except layers_to_finetune\n","\n","    Parameters\n","    ----------\n","    ModelConstructor : Callable[[bool], [str], [tuple], Sequential]\n","                       Function that download the pretrained model, i.e. one of the Keras applications:\n","                       https://keras.io/api/applications/\n","                       The arguments are include_top, weights, and input_shape.\n","    input_shape : tuple\n","                  The input shape for the pretrained model.\n","    print_summary : bool\n","                    If True prints the model summary.\n","\n","    Returns\n","    -------\n","    model : Sequential\n","          The instantiated model.\n","    \"\"\"\n","\n","    model = ModelConstructor(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n","\n","    for layer in model.layers:\n","        layer.trainable = False\n","\n","    return model\n","\n","def getLSTMModel(getConvModel, ModelConstructor, pretrained_input_shape=(224, 224, 3), verbose=True):\n","    \"\"\"Creates the BiLSTM + fully connected layers end-to-end model object\n","    with the sequential API: https://keras.io/models/sequential/\n","\n","    Parameters\n","    ----------\n","    getConvModel : Callable[Callable[[bool], [str], [tuple], Sequential], [tuple], [bool], Sequential]\n","                Function that instantiates the pretrained Convolutional model\n","                to be applied in a time distributed fashion.\n","    ModelConstructor : Callable[[bool], [str], [tuple], Sequential]\n","                       Function that download the pretrained model, i.e. one of the Keras applications:\n","                       https://keras.io/api/applications/\n","                       The arguments are include_top, weights, and input_shape.\n","    input_shape : tuple\n","                  The input shape for the pretrained model.\n","    verbose : bool\n","              if True prints the model summary (default True)\n","\n","    Returns\n","    -------\n","    model : Sequential\n","            The instantiated model\n","    \"\"\"\n","    model = Sequential()\n","    model.add(TimeDistributed(getConvModel(ModelConstructor, pretrained_input_shape, verbose), input_shape=(16, 224, 224, 3)))\n","\n","    model.add(TimeDistributed(Flatten()))\n","    model.add(Bidirectional(LSTM(units=128, return_sequences=False)))\n","    #model.add(LSTM(units=128, return_sequences=False))\n","\n","    model.add(Dropout(0.5))\n","    model.add(Dense(128, activation='relu'))\n","\n","    model.add(Dropout(0.5))\n","    model.add(Dense(1, activation='sigmoid'))\n","    if verbose:\n","        model.summary()\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    return model\n","\n","def getConvLSTMModel(getConvModel, ModelConstructor, pretrained_input_shape=(224, 224, 3), verbose=True):\n","    \"\"\"Creates the BiLSTM + fully connected layers end-to-end model object\n","    with the sequential API: https://keras.io/models/sequential/\n","\n","    Parameters\n","    ----------\n","    getConvModel : Callable[Callable[[bool], [str], [tuple], Sequential], [tuple], [bool], Sequential]\n","                Function that instantiates the pretrained Convolutional model\n","                to be applied in a time distributed fashion.\n","    ModelConstructor : Callable[[bool], [str], [tuple], Sequential]\n","                       Function that download the pretrained model, i.e. one of the Keras applications:\n","                       https://keras.io/api/applications/\n","                       The arguments are include_top, weights, and input_shape.\n","    input_shape : tuple\n","                  The input shape for the pretrained model.\n","    verbose : bool\n","              if True prints the model summary (default True)\n","\n","    Returns\n","    -------\n","    model : Sequential\n","            The instantiated model\n","    \"\"\"\n","    model = Sequential()\n","    model.add(TimeDistributed(getConvModel(ModelConstructor, pretrained_input_shape, verbose), input_shape=(16, 224, 224, 3)))\n","\n","    model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3)))\n","\n","    model.add(Flatten())\n","    model.add(Dropout(0.5))\n","    model.add(Dense(256, activation='relu'))\n","\n","    model.add(Dropout(0.5))\n","    model.add(Dense(1, activation='sigmoid'))\n","    if verbose:\n","        model.summary()\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    return model\n","\n","\n","def runEndToEndExperiment(getLSTMModel, getConvModel, ModelConstructor, pretrained_input_shape, Preprocess_input, batchSize, datasetBasePath, npyBasePath, featuresPath, samplesMMapName, lablesMMapName, endToEndModelName, rState, savePath):\n","        return  getLSTMModel(getConvModel, ModelConstructor, pretrained_input_shape)\n","\n","\n","\n","# Definisci il callback personalizzato\n","class InferenceTimeCallback(Callback):\n","    def on_predict_batch_begin(self, batch, logs=None):\n","        self.start_time = time.time()\n","\n","    def on_predict_batch_end(self, batch, logs=None):\n","        self.end_time = time.time()\n","        self.inference_time = self.end_time - self.start_time\n","        print(f\"Inference time for batch {batch}: {self.inference_time} seconds\")\n"]},{"cell_type":"code","source":["# Mounts Google Drive\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"metadata":{"id":"LVpBtpuTd35e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install moviepy\n","!pip install codecarbon"],"metadata":{"id":"n2jGLHy6dpSL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import cv2\n","import numpy as np\n","import os\n","\n","def ensure_dir(file_path):\n","    if not os.path.exists(file_path):\n","        os.makedirs(file_path)\n","\n","def compress_jpeg(image, quality):\n","    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]\n","    _, encimg = cv2.imencode('.jpg', image, encode_param)\n","    decimg = cv2.imdecode(encimg, 1)\n","    return decimg\n","\n","def reduce_resolution(image, scale_percent):\n","    width = int(image.shape[1] * scale_percent / 100)\n","    height = int(image.shape[0] * scale_percent / 100)\n","    dim = (width, height)\n","    resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)\n","    return cv2.resize(resized, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_AREA)\n","\n","def add_noise(image, mean=0, var=10):\n","    sigma = var ** 0.5\n","    gauss = np.random.normal(mean, sigma, image.shape).astype('uint8')\n","    noisy_image = cv2.add(image, gauss)\n","    return noisy_image\n","\n","def blur_image(image, kernel_size=5):\n","    return cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n","\n","def distort_color(image, alpha=1.5, beta=20):\n","    return cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n","\n","def process_and_save_video(src_path, dst_path, augmentations):\n","    cap = cv2.VideoCapture(src_path)\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    out = None\n","    if cap.isOpened():\n","        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","        fps = cap.get(cv2.CAP_PROP_FPS)\n","        ensure_dir(os.path.dirname(dst_path))\n","        out = cv2.VideoWriter(dst_path, fourcc, fps, (frame_width, frame_height))\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        for aug in augmentations:\n","            frame = aug(frame)\n","\n","        out.write(frame)\n","\n","    cap.release()\n","    out.release()\n","\n","def process_directory(src_dir, dst_dir, augmentations):\n","    categories = ['Violence', 'NonViolence']\n","    cameras = ['cam1', 'cam2']\n","\n","    for category in categories:\n","        for camera in cameras:\n","            src_path = os.path.join(src_dir, category, camera)\n","            dst_path = os.path.join(dst_dir, category, camera)\n","            ensure_dir(dst_path)\n","\n","            for filename in os.listdir(src_path):\n","                if filename.endswith(\".mp4\"):\n","                    src_video_path = os.path.join(src_path, filename)\n","                    dst_video_path = os.path.join(dst_path, filename)\n","                    process_and_save_video(src_video_path, dst_video_path, augmentations)\n","\n","\n","source_folder = '/content/gdrive/MyDrive/Dataset/Data-Augmented'\n","destination_folder = '/content/gdrive/MyDrive/Dataset/Data_Augmented'\n","\n","augmentations = [\n","    #lambda img: compress_jpeg(img, quality=70),\n","    lambda img: reduce_resolution(img, scale_percent=30),\n","    #lambda img: add_noise(img, var=20),\n","    lambda img: blur_image(img, kernel_size=25),\n","    #lambda img: distort_color(img, alpha=1.5, beta=30)\n","]\n","\n","\n","process_directory(source_folder, destination_folder, augmentations)\n"],"metadata":{"id":"XSb1m2h0fu3Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### *Pruning al 50% sul modello ConvLSTM*\n","\n","\n","\n","*   Peso modello: 12 MB\n","*   Accuratezza modello: 94.5%\n","*   Auc: 98%\n","*   Tempo di inferenza medio per batch: 0.38 s circa\n","*   Numero di parametri: 9538186\n","*   Consumo Energetico medio per 102 secondi di video: 0.0023  kW/h\n","\n"],"metadata":{"id":"3Xnx6cHSSa1E"}},{"cell_type":"code","source":["\n","\n","from tensorflow.keras.applications import MobileNetV3Small\n","from tensorflow.keras.applications.mobilenet_v3 import preprocess_input as mobilenet_v3_preprocess_input\n","from tensorflow.keras.preprocessing import image\n","import matplotlib.pyplot as plt\n","\n","\n","MobileNetV3Small_ConvLSTM = runEndToEndExperiment(\n","    getConvLSTMModel,\n","    GetPretrainedModel,\n","    MobileNetV3Small,\n","    (224, 224, 3),\n","    mobilenet_v3_preprocess_input,\n","    8,\n","    '/content/gdrive/My Drive/Dataset/AirtLab-Dataset',\n","    '/airtlabDataset',\n","    'features',\n","    'filenames.npy',\n","    'labels.npy',\n","    'MobileNetV3Small + ConvLSTM',\n","    42,\n","    '/content/gdrive/ My Drive/MobileNetV3Small_ConvLSTM'\n",")\n","\n","\n","MobileNetV3Small_ConvLSTM.load_weights('/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/ConvLSTM/final_model_fold_5.h5')\n"],"metadata":{"id":"pkaGvajDSzLr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install tensorflow-model-optimization"],"metadata":{"id":"B9Wcgy8oXzxX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","import tensorflow_model_optimization as tfmot\n","import cv2\n","\n","\n","def preprocess_video(video_path, input_shape, batch_size):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, (input_shape[1], input_shape[0]))\n","        frame = frame.astype('float32')\n","        frames.append(frame)\n","\n","    cap.release()\n","    frames = np.array(frames)\n","\n","\n","    if len(frames) >= batch_size:\n","        num_batches = len(frames) // batch_size\n","        frames = frames[:num_batches * batch_size]\n","        batches = np.split(frames, num_batches)\n","    else:\n","        batches = []\n","\n","    return batches\n","\n","def load_videos_from_folder(folder_path, label, input_shape, batch_size):\n","    videos = []\n","    labels = []\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".mp4\") or filename.endswith(\".avi\"):\n","            video_path = os.path.join(folder_path, filename)\n","            video_batches = preprocess_video(video_path, input_shape, batch_size)\n","            videos.extend(video_batches)\n","            labels.extend([label] * len(video_batches))\n","    return videos, labels\n","\n","def apply_pruning_to_supported_layers(model, pruning_params):\n","    for layer in model.layers:\n","        if isinstance(layer, (tf.keras.layers.Dense, tf.keras.layers.Conv2D)):\n","            layer = tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n","    return model\n","\n","\n","\n","prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n","train_violence_path = \"/content/gdrive/MyDrive/Dataset/Data_Augmented/Train/Violence\"\n","train_nonviolence_path = \"/content/gdrive/MyDrive/Dataset/Data_Augmented/Train/NonViolence\"\n","val_violence_path = \"/content/gdrive/MyDrive/Dataset/Data_Augmented/Validation/Violence\"\n","val_nonviolence_path = \"/content/gdrive/MyDrive/Dataset/Data_Augmented/Validation/NonViolence\"\n","input_shape = (224, 224)\n","sequence_length = 16\n","batch_size = 16\n","epochs = 2\n","\n","\n","violence_train_videos, violence_train_labels = load_videos_from_folder(train_violence_path, 1, input_shape, batch_size)\n","nonviolence_train_videos, nonviolence_train_labels = load_videos_from_folder(train_nonviolence_path, 0, input_shape, batch_size)\n","violence_val_videos, violence_val_labels = load_videos_from_folder(val_violence_path, 1, input_shape, batch_size)\n","nonviolence_val_videos, nonviolence_val_labels = load_videos_from_folder(val_nonviolence_path, 0, input_shape, batch_size)\n","\n","x_train = np.concatenate((violence_train_videos, nonviolence_train_videos), axis=0)\n","y_train = np.concatenate((violence_train_labels, nonviolence_train_labels), axis=0)\n","x_test = np.concatenate((violence_val_videos, nonviolence_val_videos), axis=0)\n","y_test = np.concatenate((violence_val_labels, nonviolence_val_labels), axis=0)\n","\n","\n","num_images = len(x_train) + len(x_test)\n","end_step = np.ceil(num_images).astype(np.int32) * epochs\n","\n","pruning_params = {\n","    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n","        initial_sparsity=0.0,\n","        final_sparsity=0.5,\n","        begin_step=0,\n","        end_step=end_step\n","    )\n","}\n","\n","\n","model_for_pruning = apply_pruning_to_supported_layers(MobileNetV3Small_ConvLSTM, pruning_params)\n","\n","\n","model_for_pruning.compile(optimizer='adam',\n","                          loss='binary_crossentropy',\n","                          metrics=['accuracy'])\n","\n","model_for_pruning.summary()\n","\n","\n","logdir = tempfile.mkdtemp()\n","\n","callbacks = [\n","    tfmot.sparsity.keras.UpdatePruningStep(),\n","    tfmot.sparsity.keras.PruningSummaries(log_dir=logdir)\n","]\n","\n","\n","model_for_pruning.fit(x_train, y_train,\n","                      batch_size=batch_size,\n","                      epochs=epochs,\n","                      callbacks=callbacks,\n","                      validation_data=(x_test, y_test))\n","\n","\n","pruned_model = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n","\n","\n","pruned_model.save('/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/Pruning/50_perc/pruned_model_ConvLSTM.h5')\n","\n","\n","converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n","\n","\n","converter.experimental_new_converter = True\n","converter._experimental_lower_tensor_list_ops = True\n","\n","\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n","                                       tf.lite.OpsSet.SELECT_TF_OPS]\n","\n","pruned_tflite_model = converter.convert()\n","\n","\n","\n","with open('/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/Pruning/50_perc/pruned_model_ConvLSTM.tflite', 'wb') as f:\n","    f.write(pruned_tflite_model)\n"],"metadata":{"id":"8PUQlJLyPR-H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","import tensorflow as tf\n","\n","\n","def tflite_model_summary(interpreter):\n","\n","    interpreter.allocate_tensors()\n","\n","\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","    all_tensor_details = interpreter.get_tensor_details()\n","\n","    layers = {}\n","\n","    for tensor_detail in all_tensor_details:\n","        layer_name = tensor_detail['name'].split('/')[0]\n","        if layer_name not in layers:\n","            layers[layer_name] = {\n","                'name': layer_name,\n","                'output_shape': [],\n","                'type': [],\n","                'param_count': 0\n","            }\n","        layers[layer_name]['output_shape'].append(tensor_detail['shape'])\n","        layers[layer_name]['type'].append(str(tensor_detail['dtype']))\n","\n","\n","    total_params = 0\n","    for tensor_detail in all_tensor_details:\n","        shape = tensor_detail['shape']\n","        param_count = 1\n","        for dim in shape:\n","            param_count *= dim\n","        total_params += param_count\n","        layer_name = tensor_detail['name'].split('/')[0]\n","        layers[layer_name]['param_count'] += param_count\n","\n","\n","    print(\"_________________________________________________________________\")\n","    print(\" Layer (type)                Output Shape              Param #   \")\n","    print(\"=================================================================\")\n","    for layer_name, layer_info in layers.items():\n","        output_shape_str = ' / '.join([str(shape) for shape in layer_info['output_shape']])\n","        dtype_str = ' / '.join(layer_info['type'])\n","        print(f\" {layer_name} ({dtype_str})  {output_shape_str}     {layer_info['param_count']}\")\n","    print(\"=================================================================\")\n","    print(f\"Total params: {total_params}\")\n","    print(\"_______________________________________________________________\")\n","\n","\n","\n","pruned_model_path = '/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/Pruning/50_perc/pruned_model_ConvLSTM.tflite'\n","interpreter = tf.lite.Interpreter(model_path=pruned_model_path)\n","\n","\n","tflite_model_summary(interpreter)\n"],"metadata":{"id":"sr7q08jad-Rb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","import cv2\n","import tensorflow as tf\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","import time\n","\n","def load_tflite_model(model_path):\n","    interpreter = tf.lite.Interpreter(model_path=model_path)\n","    interpreter.allocate_tensors()\n","    return interpreter\n","\n","def preprocess_video(video_path, input_shape, batch_size):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, (input_shape[1], input_shape[0]))\n","        frame = frame.astype('float32')\n","        frames.append(frame)\n","\n","    cap.release()\n","    frames = np.array(frames)\n","\n","\n","    if len(frames) >= batch_size:\n","        num_batches = len(frames) // batch_size\n","        frames = frames[:num_batches * batch_size]\n","        batches = np.split(frames, num_batches)\n","    else:\n","        batches = []\n","\n","    return batches\n","\n","\n","def run_inference(interpreter, input_data):\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","    interpreter.set_tensor(input_details[0]['index'], input_data)\n","\n","\n","    start_time = time.time()\n","    interpreter.invoke()\n","    end_time = time.time()\n","    inference_time = end_time - start_time\n","\n","    output_data = interpreter.get_tensor(output_details[0]['index'])\n","\n","    return output_data, inference_time\n","\n","def calculate_metrics(y_true, y_pred, y_probs):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    specificity = recall_score(y_true, y_pred, pos_label=0)\n","    fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n","    roc_auc = auc(fpr, tpr)\n","\n","    return accuracy, precision, recall, specificity, roc_auc, fpr, tpr\n","\n","def plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve'):\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='b', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='r', linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(title)\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","def load_videos_from_folder(folder_path, label, input_shape, batch_size):\n","    videos = []\n","    labels = []\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".mp4\") or filename.endswith(\".avi\"):\n","            video_path = os.path.join(folder_path, filename)\n","            video_batches = preprocess_video(video_path, input_shape, batch_size)\n","            videos.extend(video_batches)\n","            labels.extend([label] * len(video_batches))\n","    return videos, labels\n","\n","\n","from codecarbon import track_emissions\n","@track_emissions(project_name=\"MV3_Small_Inference_ConvLSTM_pruned_50\")\n","def inferenceV3_ConvLSTM():\n","    pruned_model_path = '/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/Pruning/50_perc/pruned_model_ConvLSTM.tflite'\n","    violence_path = '/content/gdrive/MyDrive/VideoInferenza/Violence'\n","    nonviolence_path = '/content/gdrive/MyDrive/VideoInferenza/NonViolence'\n","    input_shape = (224, 224)\n","    batch_size = 16\n","\n","    interpreter = load_tflite_model(pruned_model_path)\n","\n","    violence_videos, violence_labels = load_videos_from_folder(violence_path, 1, input_shape, batch_size)\n","    nonviolence_videos, nonviolence_labels = load_videos_from_folder(nonviolence_path, 0, input_shape, batch_size)\n","\n","    all_videos = violence_videos + nonviolence_videos\n","    all_labels = violence_labels + nonviolence_labels\n","\n","    all_videos = np.array([np.expand_dims(video, axis=0) for video in all_videos])\n","    all_labels = np.array(all_labels)\n","\n","    total_inference_time = 0\n","    y_probs = []\n","    for video in all_videos:\n","        output_data, inference_time = run_inference(interpreter, video)\n","        total_inference_time += inference_time\n","        y_probs.append(output_data.ravel()[0]) # / 255.0)\n","\n","    y_probs = np.array(y_probs)\n","    print(f\"Probabilità {y_probs}\")\n","    y_pred = np.round(y_probs)\n","\n","\n","\n","    print(\"Unique values in all_labels:\", np.unique(all_labels))\n","    print(\"Unique values in y_pred:\", np.unique(y_pred))\n","\n","\n","    accuracy, precision, recall, specificity, roc_auc, fpr, tpr = calculate_metrics(all_labels, y_pred, y_probs)\n","\n","\n","    average_inference_time = total_inference_time / len(all_videos)\n","\n","\n","    print(f'Accuracy: {accuracy:.4f}')\n","    print(f'Precision: {precision:.4f}')\n","    print(f'Recall: {recall:.4f}')\n","    print(f'Specificity: {specificity:.4f}')\n","    print(f'ROC AUC: {roc_auc:.4f}')\n","    print(f'Average Inference Time: {average_inference_time:.4f} seconds')\n","\n","\n","    print('Classification Report:')\n","    print(classification_report(all_labels, y_pred))\n","\n","    print('Confusion Matrix:')\n","    print(confusion_matrix(all_labels, y_pred))\n","\n","\n","    plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve for tf_model_mv3_convLStm')\n","\n","if __name__ == '__main__':\n","    inferenceV3_ConvLSTM()\n"],"metadata":{"id":"BlAaOj3bK24r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### *Pruning al 50% sul modello BiLSTM*\n","\n","\n","*   Peso modello: 114.5 MB\n","*   Accuratezza modello: 74%\n","*   Auc: 97.6%\n","*   Tempo di inferenza medio per batch: 0.37 s circa\n","*   Numero di parametri: 64470815\n","*   Consumo Energetico medio per 102 secondi di video: 0.001998  kW/h"],"metadata":{"id":"yJbjmH2zuvKO"}},{"cell_type":"code","source":["\n","from tensorflow.keras.applications import MobileNetV3Small\n","from tensorflow.keras.applications.mobilenet_v3 import preprocess_input as mobilenet_v3_preprocess_input\n","from tensorflow.keras.preprocessing import image\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.callbacks import Callback\n","\n","\n","\n","MobileNetV3Small_BiLSTM = runEndToEndExperiment(\n","    getLSTMModel,\n","    GetPretrainedModel,\n","    MobileNetV3Small,\n","    (224, 224, 3),\n","    mobilenet_v3_preprocess_input,\n","    8,\n","    '/content/gdrive/My Drive/Dataset/AirtLab-Dataset',\n","    '/airtlabDataset',\n","    'features',\n","    'filenames.npy',\n","    'labels.npy',\n","    'MobileNetV3Small + BiLSTM',\n","    42,\n","    '/content/gdrive/ My Drive/MobileNetV3Small_BiLSTM'\n",")\n","\n","\n","MobileNetV3Small_BiLSTM.load_weights('/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/BiLSTM/final_model_fold_5.h5')\n","\n"],"metadata":{"id":"ZpA9mqmDu2ZD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tensorflow_model_optimization"],"metadata":{"id":"Mpl2vcUSRBOU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","import tensorflow_model_optimization as tfmot\n","import cv2\n","\n","\n","def preprocess_video(video_path, input_shape, batch_size):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, (input_shape[1], input_shape[0]))\n","        frame = frame.astype('float32')\n","        frames.append(frame)\n","\n","    cap.release()\n","    frames = np.array(frames)\n","\n","\n","    if len(frames) >= batch_size:\n","        num_batches = len(frames) // batch_size\n","        frames = frames[:num_batches * batch_size]\n","        batches = np.split(frames, num_batches)\n","    else:\n","        batches = []\n","\n","    return batches\n","\n","def load_videos_from_folder(folder_path, label, input_shape, batch_size):\n","    videos = []\n","    labels = []\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".mp4\") or filename.endswith(\".avi\"):\n","            video_path = os.path.join(folder_path, filename)\n","            video_batches = preprocess_video(video_path, input_shape, batch_size)\n","            videos.extend(video_batches)\n","            labels.extend([label] * len(video_batches))\n","    return videos, labels\n","\n","\n","def apply_pruning_to_supported_layers(model, pruning_params):\n","    for layer in model.layers:\n","        if isinstance(layer, (tf.keras.layers.Dense, tf.keras.layers.Conv2D)):\n","            layer = tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n","    return model\n","\n","\n","prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n","train_violence_path = \"/content/gdrive/MyDrive/Dataset/Data_Augmented/Train/Violence\"\n","train_nonviolence_path = \"/content/gdrive/MyDrive/Dataset/Data_Augmented/Train/NonViolence\"\n","val_violence_path = \"/content/gdrive/MyDrive/Dataset/Data_Augmented/Validation/Violence\"\n","val_nonviolence_path = \"/content/gdrive/MyDrive/Dataset/Data_Augmented/Validation/NonViolence\"\n","input_shape = (224, 224)\n","sequence_length = 16\n","batch_size = 16\n","epochs = 2\n","\n","\n","violence_train_videos, violence_train_labels = load_videos_from_folder(train_violence_path, 1, input_shape, batch_size)\n","nonviolence_train_videos, nonviolence_train_labels = load_videos_from_folder(train_nonviolence_path, 0, input_shape, batch_size)\n","violence_val_videos, violence_val_labels = load_videos_from_folder(val_violence_path, 1, input_shape, batch_size)\n","nonviolence_val_videos, nonviolence_val_labels = load_videos_from_folder(val_nonviolence_path, 0, input_shape, batch_size)\n","\n","x_train = np.concatenate((violence_train_videos, nonviolence_train_videos), axis=0)\n","y_train = np.concatenate((violence_train_labels, nonviolence_train_labels), axis=0)\n","x_test = np.concatenate((violence_val_videos, nonviolence_val_videos), axis=0)\n","y_test = np.concatenate((violence_val_labels, nonviolence_val_labels), axis=0)\n","\n","\n","num_images = len(x_train) + len(x_test)\n","end_step = np.ceil(num_images).astype(np.int32) * epochs\n","\n","pruning_params = {\n","    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n","        initial_sparsity=0.0,\n","        final_sparsity=0.5,\n","        begin_step=0,\n","        end_step=end_step\n","    )\n","}\n","\n","\n","model_for_pruning_biLSTM = apply_pruning_to_supported_layers(MobileNetV3Small_BiLSTM, pruning_params)\n","\n","\n","model_for_pruning_biLSTM.compile(optimizer='adam',\n","                          loss='binary_crossentropy',\n","                          metrics=['accuracy'])\n","\n","model_for_pruning_biLSTM.summary()\n","\n","\n","logdir = tempfile.mkdtemp()\n","\n","callbacks = [\n","    tfmot.sparsity.keras.UpdatePruningStep(),\n","    tfmot.sparsity.keras.PruningSummaries(log_dir=logdir)\n","]\n","\n","\n","model_for_pruning_biLSTM.fit(x_train, y_train,\n","                      batch_size=batch_size,\n","                      epochs=epochs,\n","                      callbacks=callbacks,\n","                      validation_data=(x_test, y_test))\n","\n","\n","pruned_model_biLSTM = tfmot.sparsity.keras.strip_pruning(model_for_pruning_biLSTM)\n","\n","\n","pruned_model_biLSTM.save('/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/Pruning/50_perc/pruned_model_BiLSTM.h5')\n","\n","\n","converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model_biLSTM)\n","\n","\n","converter.experimental_new_converter = True\n","converter._experimental_lower_tensor_list_ops = True\n","\n","\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n","                                       tf.lite.OpsSet.SELECT_TF_OPS]\n","\n","pruned_tflite_model_biLSTM = converter.convert()\n","\n","\n","\n","with open('/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/Pruning/50_perc/pruned_model_BiLSTM.tflite', 'wb') as f:\n","    f.write(pruned_tflite_model_biLSTM)\n"],"metadata":{"id":"pFRgch2cKrXt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","import tensorflow as tf\n","\n","\n","def tflite_model_summary(interpreter):\n","\n","    interpreter.allocate_tensors()\n","\n","\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","    all_tensor_details = interpreter.get_tensor_details()\n","\n","    layers = {}\n","\n","    for tensor_detail in all_tensor_details:\n","        layer_name = tensor_detail['name'].split('/')[0]\n","        if layer_name not in layers:\n","            layers[layer_name] = {\n","                'name': layer_name,\n","                'output_shape': [],\n","                'type': [],\n","                'param_count': 0\n","            }\n","        layers[layer_name]['output_shape'].append(tensor_detail['shape'])\n","        layers[layer_name]['type'].append(str(tensor_detail['dtype']))\n","\n","\n","    total_params = 0\n","    for tensor_detail in all_tensor_details:\n","        shape = tensor_detail['shape']\n","        param_count = 1\n","        for dim in shape:\n","            param_count *= dim\n","        total_params += param_count\n","        layer_name = tensor_detail['name'].split('/')[0]\n","        layers[layer_name]['param_count'] += param_count\n","\n","\n","    print(\"_________________________________________________________________\")\n","    print(\" Layer (type)                Output Shape              Param #   \")\n","    print(\"=================================================================\")\n","    for layer_name, layer_info in layers.items():\n","        output_shape_str = ' / '.join([str(shape) for shape in layer_info['output_shape']])\n","        dtype_str = ' / '.join(layer_info['type'])\n","        print(f\" {layer_name} ({dtype_str})  {output_shape_str}     {layer_info['param_count']}\")\n","    print(\"=================================================================\")\n","    print(f\"Total params: {total_params}\")\n","    print(\"_______________________________________________________________\")\n","\n","\n","\n","pruned_model_biLSTM_path = \"/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/Pruning/50_perc/pruned_model_BiLSTM.tflite\"\n","interpreter = tf.lite.Interpreter(model_path=pruned_model_biLSTM_path)\n","\n","\n","tflite_model_summary(interpreter)\n"],"metadata":{"id":"AXknvbwFKwVM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tensorflow-model-optimization"],"metadata":{"id":"T4_-SQntSm_O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","import cv2\n","import tensorflow as tf\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","import time\n","\n","def load_tflite_model(model_path):\n","    interpreter = tf.lite.Interpreter(model_path=model_path)\n","    interpreter.allocate_tensors()\n","    return interpreter\n","\n","def preprocess_video(video_path, input_shape, batch_size):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, (input_shape[1], input_shape[0]))\n","        frame = frame.astype('float32')\n","        frames.append(frame)\n","\n","    cap.release()\n","    frames = np.array(frames)\n","\n","\n","    if len(frames) >= batch_size:\n","        num_batches = len(frames) // batch_size\n","        frames = frames[:num_batches * batch_size]\n","        batches = np.split(frames, num_batches)\n","    else:\n","        batches = []\n","\n","    return batches\n","\n","\n","def run_inference(interpreter, input_data):\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","    interpreter.set_tensor(input_details[0]['index'], input_data)\n","\n","\n","    start_time = time.time()\n","    interpreter.invoke()\n","    end_time = time.time()\n","    inference_time = end_time - start_time\n","\n","    output_data = interpreter.get_tensor(output_details[0]['index'])\n","\n","    return output_data, inference_time\n","\n","def calculate_metrics(y_true, y_pred, y_probs):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    specificity = recall_score(y_true, y_pred, pos_label=0)\n","    fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n","    roc_auc = auc(fpr, tpr)\n","\n","    return accuracy, precision, recall, specificity, roc_auc, fpr, tpr\n","\n","def plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve'):\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='b', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='r', linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(title)\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","def load_videos_from_folder(folder_path, label, input_shape, batch_size):\n","    videos = []\n","    labels = []\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".mp4\") or filename.endswith(\".avi\"):\n","            video_path = os.path.join(folder_path, filename)\n","            video_batches = preprocess_video(video_path, input_shape, batch_size)\n","            videos.extend(video_batches)\n","            labels.extend([label] * len(video_batches))\n","    return videos, labels\n","\n","\n","from codecarbon import track_emissions\n","@track_emissions(project_name=\"MV3_Small_Inference_BiLSTM_pruned_50\")\n","def inferenceV3_BiLSTM():\n","    pruned_model_biLSTM_path = '/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/Pruning/50_perc/pruned_model_BiLSTM.tflite'\n","    violence_path = '/content/gdrive/MyDrive/VideoInferenza/Violence'\n","    nonviolence_path = '/content/gdrive/MyDrive/VideoInferenza/NonViolence'\n","    input_shape = (224, 224)\n","    batch_size = 16\n","\n","    interpreter = load_tflite_model(pruned_model_biLSTM_path)\n","\n","    violence_videos, violence_labels = load_videos_from_folder(violence_path, 1, input_shape, batch_size)\n","    nonviolence_videos, nonviolence_labels = load_videos_from_folder(nonviolence_path, 0, input_shape, batch_size)\n","\n","    all_videos = violence_videos + nonviolence_videos\n","    all_labels = violence_labels + nonviolence_labels\n","\n","    all_videos = np.array([np.expand_dims(video, axis=0) for video in all_videos])\n","    all_labels = np.array(all_labels)\n","\n","    total_inference_time = 0\n","    y_probs = []\n","    for video in all_videos:\n","        output_data, inference_time = run_inference(interpreter, video)\n","        total_inference_time += inference_time\n","        y_probs.append(output_data.ravel()[0]) # / 255.0)\n","\n","    y_probs = np.array(y_probs)\n","    print(f\"Probabilità {y_probs}\")\n","    y_pred = np.round(y_probs)\n","\n","\n","\n","    print(\"Unique values in all_labels:\", np.unique(all_labels))\n","    print(\"Unique values in y_pred:\", np.unique(y_pred))\n","\n","\n","    accuracy, precision, recall, specificity, roc_auc, fpr, tpr = calculate_metrics(all_labels, y_pred, y_probs)\n","\n","\n","    average_inference_time = total_inference_time / len(all_videos)\n","\n","\n","    print(f'Accuracy: {accuracy:.4f}')\n","    print(f'Precision: {precision:.4f}')\n","    print(f'Recall: {recall:.4f}')\n","    print(f'Specificity: {specificity:.4f}')\n","    print(f'ROC AUC: {roc_auc:.4f}')\n","    print(f'Average Inference Time: {average_inference_time:.4f} seconds')\n","\n","\n","    print('Classification Report:')\n","    print(classification_report(all_labels, y_pred))\n","\n","    print('Confusion Matrix:')\n","    print(confusion_matrix(all_labels, y_pred))\n","\n","\n","    plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve for tf_model_mv3_BiLStm_prunato_50')\n","\n","if __name__ == '__main__':\n","    inferenceV3_BiLSTM()\n"],"metadata":{"id":"SwKJFv2YvGyi"},"execution_count":null,"outputs":[]}]}