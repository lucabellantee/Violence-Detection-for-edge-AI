{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["2qmDiOkgQ2DX","NjCYimNU4e3O","6l_-Vo4oQ_2k","KjZHWlC_7Lf8","yJbjmH2zuvKO","jk7ZX-PMFR3C","bw53hyqS9Nn1","2Y2yUeHnoz4A"],"mount_file_id":"1OBbpjZKUh4Xd2F6LBMLbodEGeVwsmRI6","authorship_tag":"ABX9TyMckRgMAlt1vP5Rkq1dA6UT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install codecarbon"],"metadata":{"id":"y3FLuLwdoklY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MobileNet-V3 small + convLSTM"],"metadata":{"id":"ALd3ICjgfaQJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"x9cduLOoSNQ5"},"outputs":[],"source":["# definitions of two end-to-end models + definitions of experiments\n","\n","import pandas as pd\n","import numpy as np\n","import sklearn\n","from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n","from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, classification_report\n","from keras.callbacks import EarlyStopping\n","import matplotlib.pylab as plt\n","import os\n","from keras.models import Sequential, Model\n","from keras.layers import Input, Dense, Dropout, Flatten, ConvLSTM2D, TimeDistributed, Bidirectional, LSTM\n","from keras.utils import Sequence\n","from tensorflow.keras.callbacks import Callback\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","\n","\n","# Definisce il callback personalizzato\n","class InferenceTimeCallback(Callback):\n","    def on_predict_batch_begin(self, batch, logs=None):\n","        self.start_time = time.time()\n","\n","    def on_predict_batch_end(self, batch, logs=None):\n","        self.end_time = time.time()\n","        self.inference_time = self.end_time - self.start_time\n","        print(f\"Inference time for batch {batch}: {self.inference_time} seconds\")\n","\n","\n"]},{"cell_type":"markdown","source":["### **Test sui tempi di inferenza per l'ultimo split sul modello ConvLSTM**"],"metadata":{"id":"3Xnx6cHSSa1E"}},{"cell_type":"code","source":["# Monto il drive Google\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"metadata":{"id":"1Ivj_nB8SlRh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Esporto la MobileNEt-V3 Small\n","\n","from tensorflow.keras.models import load_model\n","\n","MobileNetV3Small_ConvLSTM= load_model('/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/ConvLSTM/final_model_fold_5.h5')\n","\n","\n","# Stampo il summary\n","MobileNetV3Small_ConvLSTM.summary()\n"],"metadata":{"id":"pkaGvajDSzLr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funzioni per il pre processamento dei video e l'inferenza\n","\n","import os\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import load_model\n","import time\n","\n","def load_keras_model(model_path):\n","    model = load_model(model_path)\n","    return model\n","\n","def preprocess_video(video_path, input_shape, batch_size):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, (input_shape[1], input_shape[0]))\n","        frame = frame.astype('float32') #/ 255.0\n","        frames.append(frame)\n","\n","    cap.release()\n","    frames = np.array(frames)\n","\n","    if len(frames) >= batch_size:\n","        num_batches = len(frames) // batch_size\n","        frames = frames[:num_batches * batch_size]\n","        batches = np.split(frames, num_batches)\n","    else:\n","        batches = []\n","\n","    return batches\n","\n","def run_inference(model, input_data):\n","    output_data = model.predict(input_data)\n","    return output_data\n","\n","def calculate_metrics(y_true, y_pred, y_probs):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    specificity = recall_score(y_true, y_pred, pos_label=0)\n","    fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n","    roc_auc = auc(fpr, tpr)\n","\n","    return accuracy, precision, recall, specificity, roc_auc, fpr, tpr\n","\n","def plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve'):\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='b', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='r', linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(title)\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","def load_videos_from_folder(folder_path, label, input_shape, batch_size):\n","    videos = []\n","    labels = []\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".mp4\") or filename.endswith(\".avi\"):\n","            video_path = os.path.join(folder_path, filename)\n","            video_batches = preprocess_video(video_path, input_shape, batch_size)\n","            videos.extend(video_batches)\n","            labels.extend([label] * len(video_batches))\n","    return videos, labels\n","\n","def inferenceV3_ConvLSTM_NOquantizzata():\n","    model_path = '/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/ConvLSTM/final_model_fold_5.h5'\n","    violence_path = '/content/gdrive/MyDrive/VideoInferenza/Violence'\n","    nonviolence_path = '/content/gdrive/MyDrive/VideoInferenza/NonViolence'\n","    input_shape = (224, 224)\n","    batch_size = 16\n","\n","    model = load_keras_model(model_path)\n","\n","    violence_videos, violence_labels = load_videos_from_folder(violence_path, 1, input_shape, batch_size)\n","    nonviolence_videos, nonviolence_labels = load_videos_from_folder(nonviolence_path, 0, input_shape, batch_size)\n","\n","    all_videos = violence_videos + nonviolence_videos\n","    all_labels = violence_labels + nonviolence_labels\n","\n","    all_videos = np.array([np.expand_dims(video, axis=0) for video in all_videos])\n","    all_labels = np.array(all_labels)\n","\n","    total_inference_time = 0\n","    y_probs = []\n","    for video in all_videos:\n","        start_time = time.time()\n","        output_data = run_inference(model, video)\n","        end_time = time.time()\n","        inference_time = end_time - start_time\n","        total_inference_time += inference_time\n","        y_probs.append(output_data.ravel()[0])\n","\n","    y_probs = np.array(y_probs)\n","    y_pred = np.round(y_probs)\n","\n","    # Calculate metrics\n","    accuracy, precision, recall, specificity, roc_auc, fpr, tpr = calculate_metrics(all_labels, y_pred, y_probs)\n","\n","    # Calculate average inference time per batch\n","    average_inference_time_per_batch = total_inference_time / len(all_videos)\n","\n","    # Print metrics\n","    print(f'Accuracy: {accuracy:.4f}')\n","    print(f'Precision: {precision:.4f}')\n","    print(f'Recall: {recall:.4f}')\n","    print(f'Specificity: {specificity:.4f}')\n","    print(f'ROC AUC: {roc_auc:.4f}')\n","    print(f'Average Inference Time per Batch: {average_inference_time_per_batch:.4f} seconds')\n","\n","    # Print classification report and confusion matrix\n","    print('Classification Report:')\n","    print(classification_report(all_labels, y_pred, target_names=['NonViolent', 'Violent']))\n","\n","    print('Confusion Matrix:')\n","    print(confusion_matrix(all_labels, y_pred))\n","\n","    # Plot ROC curve\n","    plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve for tf_model_mv3_convLStm')\n","\n","if __name__ == '__main__':\n","    inferenceV3_ConvLSTM_NOquantizzata()\n"],"metadata":{"id":"zYfBNTPSHHQn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Quantizzazione intera 8 bit con fallback a float(utilizzando input/output float predefinito)\n","\n","\n","\n","*   Peso modello: circa 3.33 MB\n","*   Accuratezza modello: 87.8 %\n","*   Auc: circa 90.6 %\n","*   Tempo inferenza medio per batch: 0.33 s\n","*   Consumo Energetico medio per 102 secondi di video:  0.002  kW/h\n","\n","\n","\n","\n","\n"],"metadata":{"id":"2qmDiOkgQ2DX"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import cv2\n","import os\n","\n","\n","# Dimensioni dei batch di input alla rete\n","batch_size = 16\n","height = 224\n","width = 224\n","channels = 3\n","\n","# Funzione per estrarre frame da un video e creare i batch\n","def generate_video_batches(video_paths, seed=None):\n","    batches = []\n","    for video_path in video_paths:\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","\n","            frame = cv2.resize(frame, (width, height))\n","\n","            frames.append(frame)\n","\n","            if len(frames) == batch_size:\n","                batches.append(frames)\n","                frames = []\n","        cap.release()\n","\n","    for batch in batches:\n","        yield [np.array(batch, dtype=np.float32)]\n","\n","# Imposta il convertitore\n","converter = tf.lite.TFLiteConverter.from_keras_model(MobileNetV3Small_ConvLSTM)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.representative_dataset = lambda: generate_video_batches(['/content/gdrive/MyDrive/Dataset/AirtLab-Dataset/Violence/cam1/1.mp4'],\n","                                                                 seed=42)\n","\n","# Abilita Select TF ops e disabilita experimental lowering per tensor list ops\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","converter._experimental_lower_tensor_list_ops = False\n","\n","tflite_quant_model = converter.convert()\n","\n","with open('converted_model_ConvLSTM_Quantizzazione_Int_fallbackFloat.tflite', 'wb') as f:\n","    f.write(tflite_quant_model)\n","\n","\n","model_quantized_size = os.path.getsize('converted_model_ConvLSTM_Quantizzazione_Int_fallbackFloat.tflite')\n","model_quantized_size_mb = model_quantized_size / (1024 * 1024)\n","print(f\"Quantizzazione completata e modello salvato come 'converted_model_ConvLSTM_Quantizzazione_Int_fallbackFloat.tflite'  Peso modello: {model_quantized_size_mb} MB\")\n","\n"],"metadata":{"id":"8PUQlJLyPR-H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","def tflite_model_summary(interpreter):\n","    # Allocate tensors\n","    interpreter.allocate_tensors()\n","\n","    # Get input and output tensor details\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","    all_tensor_details = interpreter.get_tensor_details()\n","\n","    layers = {}\n","    # Details for each tensor\n","    for tensor_detail in all_tensor_details:\n","        layer_name = tensor_detail['name'].split('/')[0]\n","        if layer_name not in layers:\n","            layers[layer_name] = {\n","                'name': layer_name,\n","                'output_shape': [],\n","                'type': [],\n","                'param_count': 0\n","            }\n","        layers[layer_name]['output_shape'].append(tensor_detail['shape'])\n","        layers[layer_name]['type'].append(str(tensor_detail['dtype']))\n","\n","    # Calculate the number of parameters for each layer\n","    total_params = 0\n","    for tensor_detail in all_tensor_details:\n","        shape = tensor_detail['shape']\n","        param_count = 1\n","        for dim in shape:\n","            param_count *= dim\n","        total_params += param_count\n","        layer_name = tensor_detail['name'].split('/')[0]\n","        layers[layer_name]['param_count'] += param_count\n","\n","    # Print the summary\n","    print(\"_________________________________________________________________\")\n","    print(\" Layer (type)                Output Shape              Param #   \")\n","    print(\"=================================================================\")\n","    for layer_name, layer_info in layers.items():\n","        output_shape_str = ' / '.join([str(shape) for shape in layer_info['output_shape']])\n","        dtype_str = ' / '.join(layer_info['type'])\n","        print(f\" {layer_name} ({dtype_str})  {output_shape_str}     {layer_info['param_count']}\")\n","    print(\"=================================================================\")\n","    print(f\"Total params: {total_params}\")\n","    print(\"_______________________________________________________________\")\n","\n","interpreter = tf.lite.Interpreter(model_path=\"converted_model_ConvLSTM_Quantizzazione_Int_fallbackFloat.tflite\")\n","\n","tflite_model_summary(interpreter)\n"],"metadata":{"id":"sczpjeMA_kNA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funzioni di pre processamento dei video e inferenza\n","\n","import os\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","import time\n","\n","def load_tflite_model(model_path):\n","    interpreter = tf.lite.Interpreter(model_path=model_path)\n","    interpreter.allocate_tensors()\n","    return interpreter\n","\n","def preprocess_video(video_path, input_shape, batch_size):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, (input_shape[1], input_shape[0]))\n","        frame = frame.astype('float32')\n","        frames.append(frame)\n","\n","    cap.release()\n","    frames = np.array(frames)\n","\n","    if len(frames) >= batch_size:\n","        num_batches = len(frames) // batch_size\n","        frames = frames[:num_batches * batch_size]\n","        batches = np.split(frames, num_batches)\n","    else:\n","        batches = []\n","\n","    return batches\n","\n","def run_inference(interpreter, input_data):\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","    interpreter.set_tensor(input_details[0]['index'], input_data)\n","\n","    # Calcola il tempo di inferenza\n","    start_time = time.time()\n","    interpreter.invoke()\n","    end_time = time.time()\n","    inference_time = end_time - start_time\n","\n","    output_data = interpreter.get_tensor(output_details[0]['index'])\n","\n","    return output_data, inference_time\n","\n","def calculate_metrics(y_true, y_pred, y_probs):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    specificity = recall_score(y_true, y_pred, pos_label=0)\n","    fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n","    roc_auc = auc(fpr, tpr)\n","\n","    return accuracy, precision, recall, specificity, roc_auc, fpr, tpr\n","\n","def plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve'):\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='b', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='r', linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(title)\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","def load_videos_from_folder(folder_path, label, input_shape, batch_size):\n","    videos = []\n","    labels = []\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".mp4\") or filename.endswith(\".avi\"):\n","            video_path = os.path.join(folder_path, filename)\n","            video_batches = preprocess_video(video_path, input_shape, batch_size)\n","            videos.extend(video_batches)\n","            labels.extend([label] * len(video_batches))\n","    return videos, labels\n","\n","\n","from codecarbon import track_emissions\n","@track_emissions(project_name=\"MV3_Small_Inference_ConvLSTM_int8Bit_fallbackFLoat\")\n","def inferenceV3_ConvLSTM_int8Bit_fallbackFloat():\n","    model_path = '/content/converted_model_ConvLSTM_Quantizzazione_Int_fallbackFloat.tflite'\n","    violence_path = '/content/gdrive/MyDrive/VideoInferenza/Violence'\n","    nonviolence_path = '/content/gdrive/MyDrive/VideoInferenza/NonViolence'\n","    input_shape = (224, 224)\n","    batch_size = 16\n","\n","    interpreter = load_tflite_model(model_path)\n","\n","    violence_videos, violence_labels = load_videos_from_folder(violence_path, 1, input_shape, batch_size)\n","    nonviolence_videos, nonviolence_labels = load_videos_from_folder(nonviolence_path, 0, input_shape, batch_size)\n","\n","    all_videos = violence_videos + nonviolence_videos\n","    all_labels = violence_labels + nonviolence_labels\n","\n","    all_videos = np.array([np.expand_dims(video, axis=0) for video in all_videos])\n","    all_labels = np.array(all_labels)\n","\n","    total_inference_time = 0\n","    y_probs = []\n","    for video in all_videos:\n","        output_data, inference_time = run_inference(interpreter, video)\n","        total_inference_time += inference_time\n","        y_probs.append(output_data.ravel()[0])\n","\n","    y_probs = np.array(y_probs)\n","    y_pred = np.round(y_probs)\n","\n","    # Calculate metrics\n","    accuracy, precision, recall, specificity, roc_auc, fpr, tpr = calculate_metrics(all_labels, y_pred, y_probs)\n","\n","    # Calculate average inference time\n","    average_inference_time = total_inference_time / len(all_videos)\n","\n","    # Print metrics\n","    print(f'Accuracy: {accuracy:.4f}')\n","    print(f'Precision: {precision:.4f}')\n","    print(f'Recall: {recall:.4f}')\n","    print(f'Specificity: {specificity:.4f}')\n","    print(f'ROC AUC: {roc_auc:.4f}')\n","    print(f'Average Inference Time: {average_inference_time:.4f} seconds')\n","\n","    # Print classification report and confusion matrix\n","    print('Classification Report:')\n","    print(classification_report(all_labels, y_pred))\n","\n","    print('Confusion Matrix:')\n","    print(confusion_matrix(all_labels, y_pred))\n","\n","    # Plot ROC curve\n","    plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve for tf_model_mv3_convLStm_int8Bit_fallbackFLoat')\n","\n","if __name__ == '__main__':\n","    inferenceV3_ConvLSTM_int8Bit_fallbackFloat()\n"],"metadata":{"id":"I66q4DTlT_M8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Quantizzazione intera 8 bit\n","\n","\n","\n","*   Peso modello: circa  3.33 MB\n","*   Accuratezza modello:  51 %\n","*   Auc: circa 59 %\n","*   Tempo inferenza medio per batch: 0.32 s\n","*   Consumo Energetico medio per 102 secondi di video:  0.0018  kW/h\n","\n","**Attenzione**: Prima della riconversione a Float, alcune probabilità escono negative, questo potrebbe essere dovuto a problemi di overflow o underflow numerici, in alcuni passaggi di quantizzazione e dequantizzazione.\n","\n","\n","Si ricorda che tale fenomeno non è presente, con la stessa tipologia di quantizzazione, nel BiLSTM.\n"],"metadata":{"id":"NjCYimNU4e3O"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import cv2\n","import os\n","\n","# Dimensioni dei batch di input della rete\n","batch_size = 16\n","height = 224\n","width = 224\n","channels = 3\n","\n","# Funzione per estrarre frame da un video e creare i batch\n","def generate_video_batches(video_paths, seed=None):\n","    batches = []\n","    for video_path in video_paths:\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame = cv2.resize(frame, (width, height))\n","            frames.append(frame)\n","            if len(frames) == batch_size:\n","                batches.append(frames)\n","                frames = []\n","        cap.release()\n","\n","    for batch in batches:\n","        yield [np.array(batch, dtype=np.float32)]\n","\n","# Imposta il convertitore\n","converter = tf.lite.TFLiteConverter.from_keras_model(MobileNetV3Small_ConvLSTM)\n","\n","converter.representative_dataset = lambda: generate_video_batches(['/content/gdrive/MyDrive/Dataset/AirtLab-Dataset/Violence/cam1/1.mp4'],\n","                                                                 seed=42)\n","\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","converter.inference_input_type = tf.int8  # or tf.uint8\n","converter.inference_output_type = tf.int8  # or tf.uint8\n","\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","converter._experimental_lower_tensor_list_ops = False\n","\n","tflite_quant_model = converter.convert()\n","\n","\n","with open('converted_model_ConvLSTM_Quantizzazione_Int.tflite', 'wb') as f:\n","    f.write(tflite_quant_model)\n","\n","model_quantized_size = os.path.getsize('converted_model_ConvLSTM_Quantizzazione_Int.tflite')\n","model_quantized_size_mb = model_quantized_size / (1024 * 1024)\n","print(f\"Quantizzazione completata e modello salvato come 'converted_model_ConvLSTM_Quantizzazione_Int.tflite'  Peso modello: {model_quantized_size_mb} MB\")\n"],"metadata":{"id":"dzaS06HW4i7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","def tflite_model_summary(interpreter):\n","    # Allocate tensors\n","    interpreter.allocate_tensors()\n","\n","    # Get input and output tensor details\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","    all_tensor_details = interpreter.get_tensor_details()\n","\n","    layers = {}\n","    # Details for each tensor\n","    for tensor_detail in all_tensor_details:\n","        layer_name = tensor_detail['name'].split('/')[0]\n","        if layer_name not in layers:\n","            layers[layer_name] = {\n","                'name': layer_name,\n","                'output_shape': [],\n","                'type': [],\n","                'param_count': 0\n","            }\n","        layers[layer_name]['output_shape'].append(tensor_detail['shape'])\n","        layers[layer_name]['type'].append(str(tensor_detail['dtype']))\n","\n","    # Calculate the number of parameters for each layer\n","    total_params = 0\n","    for tensor_detail in all_tensor_details:\n","        shape = tensor_detail['shape']\n","        param_count = 1\n","        for dim in shape:\n","            param_count *= dim\n","        total_params += param_count\n","        layer_name = tensor_detail['name'].split('/')[0]\n","        layers[layer_name]['param_count'] += param_count\n","\n","    # Print the summary\n","    print(\"_________________________________________________________________\")\n","    print(\" Layer (type)                Output Shape              Param #   \")\n","    print(\"=================================================================\")\n","    for layer_name, layer_info in layers.items():\n","        output_shape_str = ' / '.join([str(shape) for shape in layer_info['output_shape']])\n","        dtype_str = ' / '.join(layer_info['type'])\n","        print(f\" {layer_name} ({dtype_str})  {output_shape_str}     {layer_info['param_count']}\")\n","    print(\"=================================================================\")\n","    print(f\"Total params: {total_params}\")\n","    print(\"_______________________________________________________________\")\n","\n","interpreter = tf.lite.Interpreter(model_path=\"converted_model_ConvLSTM_Quantizzazione_Int.tflite\")\n","\n","tflite_model_summary(interpreter)\n"],"metadata":{"id":"7lGJroEA4oX8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funzioni di pre processamento dei video e di inferenza\n","\n","import os\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","import time\n","\n","def load_tflite_model(model_path):\n","    interpreter = tf.lite.Interpreter(model_path=model_path)\n","    interpreter.allocate_tensors()\n","    return interpreter\n","\n","def preprocess_video(video_path, input_shape, batch_size):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, (input_shape[1], input_shape[0]))\n","        frame = frame.astype('int8')\n","        frames.append(frame)\n","\n","    cap.release()\n","    frames = np.array(frames)\n","\n","    if len(frames) >= batch_size:\n","        num_batches = len(frames) // batch_size\n","        frames = frames[:num_batches * batch_size]\n","        batches = np.split(frames, num_batches)\n","    else:\n","        batches = []\n","\n","    return batches\n","\n","def run_inference(interpreter, input_data):\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","    interpreter.set_tensor(input_details[0]['index'], input_data)\n","\n","    # Calcola il tempo di inferenza\n","    start_time = time.time()\n","    interpreter.invoke()\n","    end_time = time.time()\n","    inference_time = end_time - start_time\n","\n","    output_data = interpreter.get_tensor(output_details[0]['index'])\n","\n","    return output_data, inference_time\n","\n","def calculate_metrics(y_true, y_pred, y_probs):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    specificity = recall_score(y_true, y_pred, pos_label=0)\n","    fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n","    roc_auc = auc(fpr, tpr)\n","\n","    return accuracy, precision, recall, specificity, roc_auc, fpr, tpr\n","\n","def plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve'):\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='b', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='r', linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(title)\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","def load_videos_from_folder(folder_path, label, input_shape, batch_size):\n","    videos = []\n","    labels = []\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".mp4\") or filename.endswith(\".avi\"):\n","            video_path = os.path.join(folder_path, filename)\n","            video_batches = preprocess_video(video_path, input_shape, batch_size)\n","            videos.extend(video_batches)\n","            labels.extend([label] * len(video_batches))\n","    return videos, labels\n","\n","\n","#from codecarbon import track_emissions\n","#@track_emissions(project_name=\"MV3_Small_Inference_ConvLSTM_int8Bit\")\n","def inferenceV3_BiLSTM():\n","    model_path = 'converted_model_ConvLSTM_Quantizzazione_Int.tflite'\n","    violence_path = '/content/gdrive/MyDrive/VideoInferenza/Violence'\n","    nonviolence_path = '/content/gdrive/MyDrive/VideoInferenza/NonViolence'\n","    input_shape = (224, 224)\n","    batch_size = 16\n","\n","    interpreter = load_tflite_model(model_path)\n","\n","    violence_videos, violence_labels = load_videos_from_folder(violence_path, 1, input_shape, batch_size)\n","    nonviolence_videos, nonviolence_labels = load_videos_from_folder(nonviolence_path, 0, input_shape, batch_size)\n","\n","    all_videos = violence_videos + nonviolence_videos\n","    all_labels = violence_labels + nonviolence_labels\n","\n","    all_videos = np.array([np.expand_dims(video, axis=0) for video in all_videos])\n","    all_labels = np.array(all_labels)\n","\n","    total_inference_time = 0\n","    y_probs = []\n","    for video in all_videos:\n","        output_data, inference_time = run_inference(interpreter, video)\n","        total_inference_time += inference_time\n","        y_probs.append(output_data.ravel()[0] / 255.0)\n","\n","    y_probs = np.array(y_probs)\n","    print(f\"Probabilità {y_probs}\")\n","    y_pred = np.round(y_probs)\n","\n","\n","    # Verifica i valori unici\n","    print(\"Unique values in all_labels:\", np.unique(all_labels))\n","    print(\"Unique values in y_pred:\", np.unique(y_pred))\n","\n","    # Calculate metrics\n","    accuracy, precision, recall, specificity, roc_auc, fpr, tpr = calculate_metrics(all_labels, y_pred, y_probs)\n","\n","    # Calculate average inference time\n","    average_inference_time = total_inference_time / len(all_videos)\n","\n","    # Print metrics\n","    print(f'Accuracy: {accuracy:.4f}')\n","    print(f'Precision: {precision:.4f}')\n","    print(f'Recall: {recall:.4f}')\n","    print(f'Specificity: {specificity:.4f}')\n","    print(f'ROC AUC: {roc_auc:.4f}')\n","    print(f'Average Inference Time: {average_inference_time:.4f} seconds')\n","\n","    # Print classification report and confusion matrix\n","    print('Classification Report:')\n","    print(classification_report(all_labels, y_pred))\n","\n","    print('Confusion Matrix:')\n","    print(confusion_matrix(all_labels, y_pred))\n","\n","    # Plot ROC curve\n","    plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve for tf_model_mv3_convLStm_int8Bit')\n","\n","if __name__ == '__main__':\n","    inferenceV3_BiLSTM()\n","\n","\n","\n"],"metadata":{"id":"SP7yDWXB4ryJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"ypy1OdzhpUwz"}},{"cell_type":"markdown","source":["# Quantizzazione Float a 16 bit\n","\n","**ATTENZIONE**: La quantizzazione avverrà solamente in presenza di GPU. In caso di utilizzo del modello su CPU, avverrà una de-quantizzazione automatica a float-32\n","\n","\n","\n","\n","*   Peso modello: circa 6 MB\n","*   Accuratezza modello: 95 %\n","*   Auc: 99 %\n","*   Tempo inferenza medio per batch: 0.40 s\n","*   Consumo Energetico medio per 102 secondi di video:  0.0018  kW/h\n","\n","\n"],"metadata":{"id":"6l_-Vo4oQ_2k"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import cv2\n","import os\n","\n","# Dimensione dei batch in input alla rete\n","batch_size = 16\n","height = 224\n","width = 224\n","channels = 3\n","\n","# Funzione per estrarre frame da un video e creare i batch\n","def generate_video_batches(video_paths, seed=None):\n","    batches = []\n","    for video_path in video_paths:\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame = cv2.resize(frame, (width, height))\n","            frames.append(frame)\n","            if len(frames) == batch_size:\n","                batches.append(frames)\n","                frames = []\n","        cap.release()\n","\n","    for batch in batches:\n","        yield [np.array(batch, dtype=np.float32)]\n","\n","converter = tf.lite.TFLiteConverter.from_keras_model(MobileNetV3Small_ConvLSTM)\n","\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","\n","# Configurazione della quantizzazione float16\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.target_spec.supported_types = [tf.float16]\n","\n","converter._experimental_lower_tensor_list_ops = False\n","tflite_quant_model = converter.convert()\n","\n","with open('model_quantized_float16_ConvLSTM.tflite', 'wb') as f:\n","    f.write(tflite_quant_model)\n","\n","\n","model_quantized_size = os.path.getsize('model_quantized_float16_ConvLSTM.tflite')\n","model_quantized_size_mb = model_quantized_size  / (1024*1024)\n","print(f\"Quantizzazione completata e modello salvato come 'model_quantized_float16_ConvLSTM.tflite'  , Peso modello: {model_quantized_size_mb} MB\")\n"],"metadata":{"id":"zfZUwgNeRG5v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funzioni di pre processamento dei video e di inferenza\n","\n","import os\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","import time\n","\n","def load_tflite_model(model_path):\n","    interpreter = tf.lite.Interpreter(model_path=model_path)\n","    interpreter.allocate_tensors()\n","    return interpreter\n","\n","def preprocess_video(video_path, input_shape, batch_size):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, (input_shape[1], input_shape[0]))\n","        frame = frame.astype('float32')\n","        frames.append(frame)\n","\n","    cap.release()\n","    frames = np.array(frames)\n","\n","    if len(frames) >= batch_size:\n","        num_batches = len(frames) // batch_size\n","        frames = frames[:num_batches * batch_size]\n","        batches = np.split(frames, num_batches)\n","    else:\n","        batches = []\n","\n","    return batches\n","\n","def run_inference(interpreter, input_data):\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","    interpreter.set_tensor(input_details[0]['index'], input_data)\n","\n","    # Calcola il tempo di inferenza\n","    start_time = time.time()\n","    interpreter.invoke()\n","    end_time = time.time()\n","    inference_time = end_time - start_time\n","\n","    output_data = interpreter.get_tensor(output_details[0]['index'])\n","\n","    return output_data, inference_time\n","\n","def calculate_metrics(y_true, y_pred, y_probs):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    specificity = recall_score(y_true, y_pred, pos_label=0)\n","    fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n","    roc_auc = auc(fpr, tpr)\n","\n","    return accuracy, precision, recall, specificity, roc_auc, fpr, tpr\n","\n","def plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve'):\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='b', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='r', linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(title)\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","def load_videos_from_folder(folder_path, label, input_shape, batch_size):\n","    videos = []\n","    labels = []\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".mp4\") or filename.endswith(\".avi\"):\n","            video_path = os.path.join(folder_path, filename)\n","            video_batches = preprocess_video(video_path, input_shape, batch_size)\n","            videos.extend(video_batches)\n","            labels.extend([label] * len(video_batches))\n","    return videos, labels\n","\n","\n","from codecarbon import track_emissions\n","@track_emissions(project_name=\"MV3_Small_Inference_ConvLSTM_Float16\")\n","def inferenceV3_ConvLSTM():\n","    model_path = '/content/model_quantized_float16_ConvLSTM.tflite'\n","    violence_path = '/content/gdrive/MyDrive/VideoInferenza/Violence'\n","    nonviolence_path = '/content/gdrive/MyDrive/VideoInferenza/NonViolence'\n","    input_shape = (224, 224)\n","    batch_size = 16\n","\n","    interpreter = load_tflite_model(model_path)\n","\n","    violence_videos, violence_labels = load_videos_from_folder(violence_path, 1, input_shape, batch_size)\n","    nonviolence_videos, nonviolence_labels = load_videos_from_folder(nonviolence_path, 0, input_shape, batch_size)\n","\n","    all_videos = violence_videos + nonviolence_videos\n","    all_labels = violence_labels + nonviolence_labels\n","\n","    all_videos = np.array([np.expand_dims(video, axis=0) for video in all_videos])\n","    all_labels = np.array(all_labels)\n","\n","    total_inference_time = 0\n","    y_probs = []\n","    for video in all_videos:\n","        output_data, inference_time = run_inference(interpreter, video)\n","        total_inference_time += inference_time\n","        y_probs.append(output_data.ravel()[0])\n","\n","    y_probs = np.array(y_probs)\n","    y_pred = np.round(y_probs)\n","\n","    # Calculate metrics\n","    accuracy, precision, recall, specificity, roc_auc, fpr, tpr = calculate_metrics(all_labels, y_pred, y_probs)\n","\n","    # Calculate average inference time\n","    average_inference_time = total_inference_time / len(all_videos)\n","\n","    # Print metrics\n","    print(f'Accuracy: {accuracy:.4f}')\n","    print(f'Precision: {precision:.4f}')\n","    print(f'Recall: {recall:.4f}')\n","    print(f'Specificity: {specificity:.4f}')\n","    print(f'ROC AUC: {roc_auc:.4f}')\n","    print(f'Average Inference Time: {average_inference_time:.4f} seconds')\n","\n","    # Print classification report and confusion matrix\n","    print('Classification Report:')\n","    print(classification_report(all_labels, y_pred))\n","\n","    print('Confusion Matrix:')\n","    print(confusion_matrix(all_labels, y_pred))\n","\n","    # Plot ROC curve\n","    plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve for tf_model_mv3_convLStm_Float16')\n","if __name__ == '__main__':\n","    inferenceV3_ConvLSTM()\n"],"metadata":{"id":"FTSu3jcpZhIM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Quantizzazione con attivazioni a 16 bit e pesi a 8 bit (Funzione sperimentale)\n","\n","Da problemi, non funziona !!!"],"metadata":{"id":"KjZHWlC_7Lf8"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import cv2\n","import os\n","\n","# Dimensioni dei batch in input alla rete\n","batch_size = 16\n","height = 224\n","width = 224\n","channels = 3\n","\n","# Funzione per estrarre frame da un video e creare i batch\n","def generate_video_batches(video_paths, seed=None):\n","    batches = []\n","    for video_path in video_paths:\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame = cv2.resize(frame, (width, height))\n","            frames.append(frame)\n","            if len(frames) == batch_size:\n","                batches.append(frames)\n","                frames = []\n","        cap.release()\n","\n","    for batch in batches:\n","        yield [np.array(batch, dtype=np.float32)]\n","\n","\n","converter = tf.lite.TFLiteConverter.from_keras_model(MobileNetV3Small_ConvLSTM)  # O carica il modello se necessario\n","converter.representative_dataset = lambda: generate_video_batches(\n","    ['/content/gdrive/MyDrive/Dataset/AirtLab-Dataset/Violence/cam1/1.mp4',\n","     '/content/gdrive/MyDrive/Dataset/AirtLab-Dataset/NonViolence/cam1/1.mp4'\n","     ],\n","    seed=42\n",")\n","\n","# Configurazione delle ottimizzazioni e operazioni supportate\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.target_spec.supported_ops = [\n","    tf.lite.OpsSet.TFLITE_BUILTINS,\n","    tf.lite.OpsSet.SELECT_TF_OPS,\n","    tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\n","]\n","converter._experimental_lower_tensor_list_ops = False\n","\n","\n","try:\n","    tflite_quant_model = converter.convert()\n","    with open('model_quantized_16BitActivation_8BitWeights.tflite', 'wb') as f:\n","        f.write(tflite_quant_model)\n","\n","    model_quantized_size = os.path.getsize('model_quantized_16BitActivation_8BitWeights.tflite')\n","    model_quantized_size_mb = model_quantized_size / (1024 * 1024)\n","    print(f\"Quantizzazione completata e modello salvato come 'model_quantized_16BitActivation_8BitWeights.tflite', Peso modello: {model_quantized_size_mb:.2f} MB\")\n","except Exception as e:\n","    print(\"Errore nella conversione:\", e)\n"],"metadata":{"id":"lg37E9W6eqTK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Test sui tempi di inferenza per l'ultimo split sul modello BiLSTM"],"metadata":{"id":"yJbjmH2zuvKO"}},{"cell_type":"code","source":["# Esporto la MobileNEt-V3 Small\n","\n","from tensorflow.keras.models import load_model\n","\n","\n","MobileNetV3Small_BiLSTM= load_model('/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/BiLSTM/final_model_fold_5.h5')\n","\n","MobileNetV3Small_BiLSTM.summary()\n"],"metadata":{"id":"ZpA9mqmDu2ZD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import load_model\n","import time\n","\n","def load_keras_model(model_path):\n","    model = load_model(model_path)\n","    return model\n","\n","def preprocess_video(video_path, input_shape, batch_size):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, (input_shape[1], input_shape[0]))\n","        frame = frame.astype('float32') #/ 255.0\n","        frames.append(frame)\n","\n","    cap.release()\n","    frames = np.array(frames)\n","\n","    # Divide frames into batches of batch_size\n","    if len(frames) >= batch_size:\n","        num_batches = len(frames) // batch_size\n","        frames = frames[:num_batches * batch_size]  # Truncate to a multiple of batch_size\n","        batches = np.split(frames, num_batches)\n","    else:\n","        # If there are fewer than batch_size frames, ignore this video\n","        batches = []\n","\n","    return batches\n","\n","def run_inference(model, input_data):\n","    output_data = model.predict(input_data)\n","    return output_data\n","\n","def calculate_metrics(y_true, y_pred, y_probs):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    specificity = recall_score(y_true, y_pred, pos_label=0)\n","    fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n","    roc_auc = auc(fpr, tpr)\n","\n","    return accuracy, precision, recall, specificity, roc_auc, fpr, tpr\n","\n","def plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve'):\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='b', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='r', linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(title)\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","def load_videos_from_folder(folder_path, label, input_shape, batch_size):\n","    videos = []\n","    labels = []\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".mp4\") or filename.endswith(\".avi\"):\n","            video_path = os.path.join(folder_path, filename)\n","            video_batches = preprocess_video(video_path, input_shape, batch_size)\n","            videos.extend(video_batches)\n","            labels.extend([label] * len(video_batches))\n","    return videos, labels\n","\n","def inferenceV3_BiLstm_NOquantizzata():\n","    model_path = '/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/BiLSTM/final_model_fold_5.h5'\n","    violence_path = '/content/gdrive/MyDrive/VideoInferenza/Violence'\n","    nonviolence_path = '/content/gdrive/MyDrive/VideoInferenza/NonViolence'\n","    input_shape = (224, 224)  # Change this to match your model's expected input shape\n","    batch_size = 16\n","\n","    model = load_keras_model(model_path)\n","\n","    violence_videos, violence_labels = load_videos_from_folder(violence_path, 1, input_shape, batch_size)\n","    nonviolence_videos, nonviolence_labels = load_videos_from_folder(nonviolence_path, 0, input_shape, batch_size)\n","\n","    all_videos = violence_videos + nonviolence_videos\n","    all_labels = violence_labels + nonviolence_labels\n","\n","    all_videos = np.array([np.expand_dims(video, axis=0) for video in all_videos])\n","    all_labels = np.array(all_labels)\n","\n","    total_inference_time = 0\n","    y_probs = []\n","    for video in all_videos:\n","        start_time = time.time()\n","        output_data = run_inference(model, video)\n","        end_time = time.time()\n","        inference_time = end_time - start_time\n","        total_inference_time += inference_time\n","        y_probs.append(output_data.ravel()[0])\n","\n","    y_probs = np.array(y_probs)\n","    y_pred = np.round(y_probs)\n","\n","    # Calculate metrics\n","    accuracy, precision, recall, specificity, roc_auc, fpr, tpr = calculate_metrics(all_labels, y_pred, y_probs)\n","\n","    # Calculate average inference time per batch\n","    average_inference_time_per_batch = total_inference_time / len(all_videos)\n","\n","    # Print metrics\n","    print(f'Accuracy: {accuracy:.4f}')\n","    print(f'Precision: {precision:.4f}')\n","    print(f'Recall: {recall:.4f}')\n","    print(f'Specificity: {specificity:.4f}')\n","    print(f'ROC AUC: {roc_auc:.4f}')\n","    print(f'Average Inference Time per Batch: {average_inference_time_per_batch:.4f} seconds')\n","\n","    # Print classification report and confusion matrix\n","    print('Classification Report:')\n","    print(classification_report(all_labels, y_pred, target_names=['NonViolent', 'Violent']))\n","\n","    print('Confusion Matrix:')\n","    print(confusion_matrix(all_labels, y_pred))\n","\n","    # Plot ROC curve\n","    plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve for tf_model_mv3_convLStm')\n","\n","if __name__ == '__main__':\n","    inferenceV3_BiLstm_NOquantizzata()"],"metadata":{"id":"O_NFr274N-Gk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Quantizzazione intera-intera 8 bit\n","\n","*   Peso modello: circa  28.9 MB\n","*   Accuratezza modello: 51 %\n","*   Auc: 74.6 %\n","*   Tempo inferenza medio per batch:  0.42 s   \n","*   Consumo Energetico medio per 102 secondi di video:  0.0018  kW/h\n","\n","\n","\n","\n","Per capire quali operazioni e/o layer possono essere quantizzati a 8 bit, si può far riferimento al codice sorgente presente al link: [quantizzazioni ammesse](https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_quantize_registry.py)\n","\n"],"metadata":{"id":"jk7ZX-PMFR3C"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import cv2\n","import os\n","\n","# Dimensioni dei batch in input alla rete\n","batch_size = 16\n","height = 224\n","width = 224\n","channels = 3\n","\n","# Funzione per estrarre frame da un video e creare i batch\n","def generate_video_batches(video_paths, seed=None):\n","    batches = []\n","    for video_path in video_paths:\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame = cv2.resize(frame, (width, height))\n","            frames.append(frame)\n","            if len(frames) == batch_size:\n","                batches.append(frames)\n","                frames = []\n","        cap.release()\n","\n","\n","    for batch in batches:\n","        yield [np.array(batch, dtype=np.float32)]\n","\n","converter = tf.lite.TFLiteConverter.from_keras_model(MobileNetV3Small_BiLSTM)\n","\n","converter.representative_dataset = lambda: generate_video_batches(['/content/gdrive/MyDrive/Dataset/AirtLab-Dataset/Violence/cam1/1.mp4'],\n","                                                                 seed=42)\n","\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","converter.inference_input_type = tf.int8\n","converter.inference_output_type = tf.int8\n","\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","converter._experimental_lower_tensor_list_ops = False\n","\n","tflite_quant_model = converter.convert()\n","\n","with open('converted_model_BiLSTM_Quantizzazione_Int.tflite', 'wb') as f:\n","    f.write(tflite_quant_model)\n","\n","model_quantized_size = os.path.getsize('converted_model_BiLSTM_Quantizzazione_Int.tflite')\n","model_quantized_size_mb = model_quantized_size / (1024 * 1024)\n","print(f\"Quantizzazione completata e modello salvato come 'converted_model_BiLSTM_Quantizzazione_Int.tflite'  Peso modello: {model_quantized_size_mb} MB\")\n"],"metadata":{"id":"Bpi-RspbFeO4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","def tflite_model_summary(interpreter):\n","    # Allocate tensors\n","    interpreter.allocate_tensors()\n","\n","    # Get input and output tensor details\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","    all_tensor_details = interpreter.get_tensor_details()\n","\n","    layers = {}\n","    # Details for each tensor\n","    for tensor_detail in all_tensor_details:\n","        layer_name = tensor_detail['name'].split('/')[0]\n","        if layer_name not in layers:\n","            layers[layer_name] = {\n","                'name': layer_name,\n","                'output_shape': [],\n","                'type': [],\n","                'param_count': 0\n","            }\n","        layers[layer_name]['output_shape'].append(tensor_detail['shape'])\n","        layers[layer_name]['type'].append(str(tensor_detail['dtype']))\n","\n","    # Calculate the number of parameters for each layer\n","    total_params = 0\n","    for tensor_detail in all_tensor_details:\n","        shape = tensor_detail['shape']\n","        param_count = 1\n","        for dim in shape:\n","            param_count *= dim\n","        total_params += param_count\n","        layer_name = tensor_detail['name'].split('/')[0]\n","        layers[layer_name]['param_count'] += param_count\n","\n","    # Print the summary\n","    print(\"_________________________________________________________________\")\n","    print(\" Layer (type)                Output Shape              Param #   \")\n","    print(\"=================================================================\")\n","    for layer_name, layer_info in layers.items():\n","        output_shape_str = ' / '.join([str(shape) for shape in layer_info['output_shape']])\n","        dtype_str = ' / '.join(layer_info['type'])\n","        print(f\" {layer_name} ({dtype_str})  {output_shape_str}     {layer_info['param_count']}\")\n","    print(\"=================================================================\")\n","    print(f\"Total params: {total_params}\")\n","    print(\"_______________________________________________________________\")\n","\n","interpreter = tf.lite.Interpreter(model_path=\"converted_model_BiLSTM_Quantizzazione_Int.tflite\")\n","\n","tflite_model_summary(interpreter)\n"],"metadata":{"id":"Xo50nmxaFxd7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funzioni di pre processamento dei video e di inferenza\n","\n","import os\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","import time\n","\n","def load_tflite_model(model_path):\n","    interpreter = tf.lite.Interpreter(model_path=model_path)\n","    interpreter.allocate_tensors()\n","    return interpreter\n","\n","def preprocess_video(video_path, input_shape, batch_size):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, (input_shape[1], input_shape[0]))\n","        frame = frame.astype('int8')\n","        frames.append(frame)\n","\n","    cap.release()\n","    frames = np.array(frames)\n","\n","    if len(frames) >= batch_size:\n","        num_batches = len(frames) // batch_size\n","        frames = frames[:num_batches * batch_size]\n","        batches = np.split(frames, num_batches)\n","    else:\n","        batches = []\n","\n","    return batches\n","\n","def run_inference(interpreter, input_data):\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","    interpreter.set_tensor(input_details[0]['index'], input_data)\n","\n","    # Calcola il tempo di inferenza\n","    start_time = time.time()\n","    interpreter.invoke()\n","    end_time = time.time()\n","    inference_time = end_time - start_time\n","\n","    output_data = interpreter.get_tensor(output_details[0]['index'])\n","\n","    return output_data, inference_time\n","\n","def calculate_metrics(y_true, y_pred, y_probs):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    specificity = recall_score(y_true, y_pred, pos_label=0)\n","    fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n","    roc_auc = auc(fpr, tpr)\n","\n","    return accuracy, precision, recall, specificity, roc_auc, fpr, tpr\n","\n","def plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve'):\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='b', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='r', linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(title)\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","def load_videos_from_folder(folder_path, label, input_shape, batch_size):\n","    videos = []\n","    labels = []\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".mp4\") or filename.endswith(\".avi\"):\n","            video_path = os.path.join(folder_path, filename)\n","            video_batches = preprocess_video(video_path, input_shape, batch_size)\n","            videos.extend(video_batches)\n","            labels.extend([label] * len(video_batches))\n","    return videos, labels\n","\n","\n","\n","from codecarbon import track_emissions\n","@track_emissions(project_name=\"MV3_Small_Inference_ConvLSTM_int8Bit\")\n","def inferenceV3_BiLSTM():\n","    model_path = 'converted_model_BiLSTM_Quantizzazione_Int.tflite'\n","    violence_path = '/content/gdrive/MyDrive/VideoInferenza/Violence'\n","    nonviolence_path = '/content/gdrive/MyDrive/VideoInferenza/NonViolence'\n","    input_shape = (224, 224)\n","    batch_size = 16\n","\n","    interpreter = load_tflite_model(model_path)\n","\n","    violence_videos, violence_labels = load_videos_from_folder(violence_path, 1, input_shape, batch_size)\n","    nonviolence_videos, nonviolence_labels = load_videos_from_folder(nonviolence_path, 0, input_shape, batch_size)\n","\n","    all_videos = violence_videos + nonviolence_videos\n","    all_labels = violence_labels + nonviolence_labels\n","\n","    all_videos = np.array([np.expand_dims(video, axis=0) for video in all_videos])\n","    all_labels = np.array(all_labels)\n","\n","    total_inference_time = 0\n","    y_probs = []\n","    for video in all_videos:\n","        output_data, inference_time = run_inference(interpreter, video)\n","        total_inference_time += inference_time\n","        y_probs.append(output_data.ravel()[0] / 255.0)\n","\n","    y_probs = np.array(y_probs)\n","    print(f\"Probabilità {y_probs}\")\n","    y_pred = np.round(y_probs)\n","\n","\n","    # Calculate metrics\n","    accuracy, precision, recall, specificity, roc_auc, fpr, tpr = calculate_metrics(all_labels, y_pred, y_probs)\n","\n","    # Calculate average inference time\n","    average_inference_time = total_inference_time / len(all_videos)\n","\n","    # Print metrics\n","    print(f'Accuracy: {accuracy:.4f}')\n","    print(f'Precision: {precision:.4f}')\n","    print(f'Recall: {recall:.4f}')\n","    print(f'Specificity: {specificity:.4f}')\n","    print(f'ROC AUC: {roc_auc:.4f}')\n","    print(f'Average Inference Time: {average_inference_time:.4f} seconds')\n","\n","    # Print classification report and confusion matrix\n","    print('Classification Report:')\n","    print(classification_report(all_labels, y_pred))\n","\n","    print('Confusion Matrix:')\n","    print(confusion_matrix(all_labels, y_pred))\n","\n","    # Plot ROC curve\n","    plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve for tf_model_mv3_BiLSTM_int8Bit')\n","\n","if __name__ == '__main__':\n","    inferenceV3_BiLSTM()\n","\n"],"metadata":{"id":"Ge9VlGbiGTIs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Quantizzazione intera-intera 8 bit con fallback a Float(Input ed Output sono Float)\n","\n","*   Peso modello: circa   28.9 MB  \n","*   Accuratezza modello:  80 %\n","*   Auc:  83 %\n","*   Tempo inferenza medio per batch: 0.408  s\n","*   Consumo Energetico medio per 102 secondi di video:  0.0018  kW/h\n","\n","\n"],"metadata":{"id":"bw53hyqS9Nn1"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import cv2\n","import os\n","\n","\n","# Dimensioni dei batch di input alla rete\n","batch_size = 16\n","height = 224\n","width = 224\n","channels = 3\n","\n","# Funzione per estrarre frame da un video e creare i batch\n","def generate_video_batches(video_paths, seed=None):\n","    batches = []\n","    for video_path in video_paths:\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame = cv2.resize(frame, (width, height))\n","            frames.append(frame)\n","            if len(frames) == batch_size:\n","                batches.append(frames)\n","                frames = []\n","        cap.release()\n","\n","    for batch in batches:\n","        yield [np.array(batch, dtype=np.float32)]\n","\n","converter = tf.lite.TFLiteConverter.from_keras_model(MobileNetV3Small_BiLSTM)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.representative_dataset = lambda: generate_video_batches(['/content/gdrive/MyDrive/Dataset/AirtLab-Dataset/Violence/cam1/1.mp4'],\n","                                                                 seed=42)\n","\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","converter._experimental_lower_tensor_list_ops = False\n","\n","tflite_quant_model = converter.convert()\n","\n","with open('model_quantized_BiLSTM_8bit_fallback_Float.tflite', 'wb') as f:\n","    f.write(tflite_quant_model)\n","\n","\n","model_quantized_size = os.path.getsize('model_quantized_BiLSTM_8bit_fallback_Float.tflite')\n","model_quantized_size_mb = model_quantized_size / (1024 * 1024)\n","print(f\"Quantizzazione completata e modello salvato come 'model_quantized_BiLSTM_8bit_fallback_Float.tflite'  ,   peso = {model_quantized_size_mb} MB\")\n"],"metadata":{"id":"xx2LHDbY9ZGO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funzioni di preprocessamento e di inferenza\n","\n","import os\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","import time\n","\n","def load_tflite_model(model_path):\n","    interpreter = tf.lite.Interpreter(model_path=model_path)\n","    interpreter.allocate_tensors()\n","    return interpreter\n","\n","def preprocess_video(video_path, input_shape, batch_size):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, (input_shape[1], input_shape[0]))\n","        frame = frame.astype('float32')\n","        frames.append(frame)\n","\n","    cap.release()\n","    frames = np.array(frames)\n","\n","    if len(frames) >= batch_size:\n","        num_batches = len(frames) // batch_size\n","        frames = frames[:num_batches * batch_size]\n","        batches = np.split(frames, num_batches)\n","    else:\n","        batches = []\n","\n","    return batches\n","\n","\n","def run_inference(interpreter, input_data):\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","    interpreter.set_tensor(input_details[0]['index'], input_data)\n","\n","    # Calcola il tempo di inferenza\n","    start_time = time.time()\n","    interpreter.invoke()\n","    end_time = time.time()\n","    inference_time = end_time - start_time\n","\n","    output_data = interpreter.get_tensor(output_details[0]['index'])\n","\n","    return output_data, inference_time\n","\n","def calculate_metrics(y_true, y_pred, y_probs):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    specificity = recall_score(y_true, y_pred, pos_label=0)\n","    fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n","    roc_auc = auc(fpr, tpr)\n","\n","    return accuracy, precision, recall, specificity, roc_auc, fpr, tpr\n","\n","def plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve'):\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='b', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='r', linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(title)\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","def load_videos_from_folder(folder_path, label, input_shape, batch_size):\n","    videos = []\n","    labels = []\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".mp4\") or filename.endswith(\".avi\"):\n","            video_path = os.path.join(folder_path, filename)\n","            video_batches = preprocess_video(video_path, input_shape, batch_size)\n","            videos.extend(video_batches)\n","            labels.extend([label] * len(video_batches))\n","    return videos, labels\n","\n","\n","from codecarbon import track_emissions\n","@track_emissions(project_name=\"MV3_Small_Inference_ConvLSTM_int8Bit_FallBackFloat\")\n","def inferenceV3_BiLSTM():\n","    model_path = '/content/model_quantized_BiLSTM_8bit_fallback_Float.tflite'\n","    violence_path = '/content/gdrive/MyDrive/VideoInferenza/Violence'\n","    nonviolence_path = '/content/gdrive/MyDrive/VideoInferenza/NonViolence'\n","    input_shape = (224, 224)\n","    batch_size = 16\n","\n","    interpreter = load_tflite_model(model_path)\n","\n","    violence_videos, violence_labels = load_videos_from_folder(violence_path, 1, input_shape, batch_size)\n","    nonviolence_videos, nonviolence_labels = load_videos_from_folder(nonviolence_path, 0, input_shape, batch_size)\n","\n","    all_videos = violence_videos + nonviolence_videos\n","    all_labels = violence_labels + nonviolence_labels\n","\n","    all_videos = np.array([np.expand_dims(video, axis=0) for video in all_videos])\n","    all_labels = np.array(all_labels)\n","\n","    total_inference_time = 0\n","    y_probs = []\n","    for video in all_videos:\n","        output_data, inference_time = run_inference(interpreter, video)\n","        total_inference_time += inference_time\n","        y_probs.append(output_data.ravel()[0])\n","\n","    y_probs = np.array(y_probs)\n","    y_pred = np.round(y_probs)\n","\n","    # Calculate metrics\n","    accuracy, precision, recall, specificity, roc_auc, fpr, tpr = calculate_metrics(all_labels, y_pred, y_probs)\n","\n","    # Calculate average inference time\n","    average_inference_time = total_inference_time / len(all_videos)\n","\n","    # Print metrics\n","    print(f'Accuracy: {accuracy:.4f}')\n","    print(f'Precision: {precision:.4f}')\n","    print(f'Recall: {recall:.4f}')\n","    print(f'Specificity: {specificity:.4f}')\n","    print(f'ROC AUC: {roc_auc:.4f}')\n","    print(f'Average Inference Time: {average_inference_time:.4f} seconds')\n","\n","    # Print classification report and confusion matrix\n","    print('Classification Report:')\n","    print(classification_report(all_labels, y_pred))\n","\n","    print('Confusion Matrix:')\n","    print(confusion_matrix(all_labels, y_pred))\n","\n","    # Plot ROC curve\n","    plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve for tf_model_mv3_BiLStm_int8Bit_fallbackFloat')\n","\n","if __name__ == '__main__':\n","    inferenceV3_BiLSTM()\n"],"metadata":{"id":"vCJnyqjg9ecB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Quantizzazione Float 16\n","\n","**Attenzione**: Eseguire solo con la GPU, altrimenti dequantizzerà a Float-32\n","\n","\n","*   Peso modello: circa  57 MB\n","*   Accuratezza modello:  90 %\n","*   Auc:  97.7 %\n","*   Tempo inferenza medio per batch:  0.278 s\n","*   Consumo Energetico medio per 102 secondi di video:  0.0017  kW/h\n","\n"],"metadata":{"id":"2Y2yUeHnoz4A"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import cv2\n","import os\n","\n","# Dimensioni dei batch in input alla rete\n","batch_size = 16\n","height = 224\n","width = 224\n","channels = 3\n","\n","# Funzione per estrarre frame da un video e creare i batch\n","def generate_video_batches(video_paths, seed=None):\n","    batches = []\n","    for video_path in video_paths:\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame = cv2.resize(frame, (width, height))\n","            frames.append(frame)\n","            if len(frames) == batch_size:\n","                batches.append(frames)\n","                frames = []\n","        cap.release()\n","\n","    for batch in batches:\n","        yield [np.array(batch, dtype=np.float32)]\n","\n","converter = tf.lite.TFLiteConverter.from_keras_model(MobileNetV3Small_BiLSTM)\n","\n","converter.representative_dataset = lambda: generate_video_batches(['/content/gdrive/MyDrive/Dataset/AirtLab-Dataset/Violence/cam1/1.mp4'],\n","                                                                 seed=42)\n","\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.target_spec.supported_types = [tf.float16]\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","converter._experimental_lower_tensor_list_ops = False\n","\n","tflite_quant_model = converter.convert()\n","\n","with open('converted_model_BiLSTM_Quantizzazione_Float16.tflite', 'wb') as f:\n","    f.write(tflite_quant_model)\n","\n","model_quantized_size = os.path.getsize('converted_model_BiLSTM_Quantizzazione_Float16.tflite')\n","model_quantized_size_mb = model_quantized_size / (1024 * 1024)\n","print(f\"Quantizzazione completata e modello salvato come 'converted_model_BiLSTM_Quantizzazione_Float16.tflite'  Peso modello: {model_quantized_size_mb} MB\")\n"],"metadata":{"id":"n3PUuZ2RpKsK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","def tflite_model_summary(interpreter):\n","    # Allocate tensors\n","    interpreter.allocate_tensors()\n","\n","    # Get input and output tensor details\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","    all_tensor_details = interpreter.get_tensor_details()\n","\n","    layers = {}\n","    # Details for each tensor\n","    for tensor_detail in all_tensor_details:\n","        layer_name = tensor_detail['name'].split('/')[0]\n","        if layer_name not in layers:\n","            layers[layer_name] = {\n","                'name': layer_name,\n","                'output_shape': [],\n","                'type': [],\n","                'param_count': 0\n","            }\n","        layers[layer_name]['output_shape'].append(tensor_detail['shape'])\n","        layers[layer_name]['type'].append(str(tensor_detail['dtype']))\n","\n","    # Calculate the number of parameters for each layer\n","    total_params = 0\n","    for tensor_detail in all_tensor_details:\n","        shape = tensor_detail['shape']\n","        param_count = 1\n","        for dim in shape:\n","            param_count *= dim\n","        total_params += param_count\n","        layer_name = tensor_detail['name'].split('/')[0]\n","        layers[layer_name]['param_count'] += param_count\n","\n","    # Print the summary\n","    print(\"_________________________________________________________________\")\n","    print(\" Layer (type)                Output Shape              Param #   \")\n","    print(\"=================================================================\")\n","    for layer_name, layer_info in layers.items():\n","        output_shape_str = ' / '.join([str(shape) for shape in layer_info['output_shape']])\n","        dtype_str = ' / '.join(layer_info['type'])\n","        print(f\" {layer_name} ({dtype_str})  {output_shape_str}     {layer_info['param_count']}\")\n","    print(\"=================================================================\")\n","    print(f\"Total params: {total_params}\")\n","    print(\"_______________________________________________________________\")\n","\n","interpreter = tf.lite.Interpreter(model_path=\"converted_model_BiLSTM_Quantizzazione_Float16.tflite\")\n","\n","tflite_model_summary(interpreter)\n"],"metadata":{"id":"n3_eptfQqF8j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funzioni di preprocessamento e di inferenza\n","\n","import os\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","import time\n","\n","def load_tflite_model(model_path):\n","    interpreter = tf.lite.Interpreter(model_path=model_path)\n","    interpreter.allocate_tensors()\n","    return interpreter\n","\n","def preprocess_video(video_path, input_shape, batch_size):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, (input_shape[1], input_shape[0]))\n","        frame = frame.astype('float32')\n","        frames.append(frame)\n","\n","    cap.release()\n","    frames = np.array(frames)\n","\n","    if len(frames) >= batch_size:\n","        num_batches = len(frames) // batch_size\n","        frames = frames[:num_batches * batch_size]\n","        batches = np.split(frames, num_batches)\n","    else:\n","        batches = []\n","\n","    return batches\n","\n","def run_inference(interpreter, input_data):\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","    interpreter.set_tensor(input_details[0]['index'], input_data)\n","\n","    # Calcola il tempo di inferenza\n","    start_time = time.time()\n","    interpreter.invoke()\n","    end_time = time.time()\n","    inference_time = end_time - start_time\n","\n","    output_data = interpreter.get_tensor(output_details[0]['index'])\n","\n","    return output_data, inference_time\n","\n","def calculate_metrics(y_true, y_pred, y_probs):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    specificity = recall_score(y_true, y_pred, pos_label=0)\n","    fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n","    roc_auc = auc(fpr, tpr)\n","\n","    return accuracy, precision, recall, specificity, roc_auc, fpr, tpr\n","\n","def plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve'):\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='b', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='r', linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(title)\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","def load_videos_from_folder(folder_path, label, input_shape, batch_size):\n","    videos = []\n","    labels = []\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".mp4\") or filename.endswith(\".avi\"):\n","            video_path = os.path.join(folder_path, filename)\n","            video_batches = preprocess_video(video_path, input_shape, batch_size)\n","            videos.extend(video_batches)\n","            labels.extend([label] * len(video_batches))\n","    return videos, labels\n","\n","\n","from codecarbon import track_emissions\n","@track_emissions(project_name=\"MV3_Small_Inference_BiLSTM_Float16\")\n","def inferenceV3_BiLSTM():\n","    model_path = '/content/converted_model_BiLSTM_Quantizzazione_Float16.tflite'\n","    violence_path = '/content/gdrive/MyDrive/VideoInferenza/Violence'\n","    nonviolence_path = '/content/gdrive/MyDrive/VideoInferenza/NonViolence'\n","    input_shape = (224, 224)\n","    batch_size = 16\n","\n","    interpreter = load_tflite_model(model_path)\n","\n","    violence_videos, violence_labels = load_videos_from_folder(violence_path, 1, input_shape, batch_size)\n","    nonviolence_videos, nonviolence_labels = load_videos_from_folder(nonviolence_path, 0, input_shape, batch_size)\n","\n","    all_videos = violence_videos + nonviolence_videos\n","    all_labels = violence_labels + nonviolence_labels\n","\n","    all_videos = np.array([np.expand_dims(video, axis=0) for video in all_videos])\n","    all_labels = np.array(all_labels)\n","\n","    total_inference_time = 0\n","    y_probs = []\n","    for video in all_videos:\n","        output_data, inference_time = run_inference(interpreter, video)\n","        total_inference_time += inference_time\n","        y_probs.append(output_data.ravel()[0])\n","\n","    y_probs = np.array(y_probs)\n","    y_pred = np.round(y_probs)\n","\n","    # Calculate metrics\n","    accuracy, precision, recall, specificity, roc_auc, fpr, tpr = calculate_metrics(all_labels, y_pred, y_probs)\n","\n","    # Calculate average inference time\n","    average_inference_time = total_inference_time / len(all_videos)\n","\n","    # Print metrics\n","    print(f'Accuracy: {accuracy:.4f}')\n","    print(f'Precision: {precision:.4f}')\n","    print(f'Recall: {recall:.4f}')\n","    print(f'Specificity: {specificity:.4f}')\n","    print(f'ROC AUC: {roc_auc:.4f}')\n","    print(f'Average Inference Time: {average_inference_time:.4f} seconds')\n","\n","    # Print classification report and confusion matrix\n","    print('Classification Report:')\n","    print(classification_report(all_labels, y_pred))\n","\n","    print('Confusion Matrix:')\n","    print(confusion_matrix(all_labels, y_pred))\n","\n","    # Plot ROC curve\n","    plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve for tf_model_mv3_convLStm')\n","\n","if __name__ == '__main__':\n","    inferenceV3_BiLSTM()\n"],"metadata":{"id":"f6oa_KohsEfC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"yqznvYcsxrro"}},{"cell_type":"markdown","source":["# Solo numeri interi: attivazioni a 16 bit con pesi a 8 bit (sperimentale)\n","\n","**Attenzione**: Non funziona e da problemi, probabilmente è un errore di qualche funzione di libreria, essendo una funzione ancora sperimentale\n"],"metadata":{"id":"KW-lE5AHxxae"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import cv2\n","import os\n","\n","# Dimensioni dei batch di input alle reti\n","batch_size = 16\n","height = 224\n","width = 224\n","channels = 3\n","\n","# Funzione per estrarre frame da un video e creare i batch\n","def generate_video_batches(video_paths, seed=None):\n","    batches = []\n","    for video_path in video_paths:\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame = cv2.resize(frame, (width, height))\n","            frames.append(frame)\n","            if len(frames) == batch_size:\n","                batches.append(frames)\n","                frames = []\n","        cap.release()\n","\n","    for batch in batches:\n","        yield [np.array(batch, dtype=np.float32)]\n","\n","converter = tf.lite.TFLiteConverter.from_keras_model(MobileNetV3Small_BiLSTM)\n","\n","converter.representative_dataset = lambda: generate_video_batches(['/content/gdrive/MyDrive/Dataset/AirtLab-Dataset/Violence/cam1/1.mp4'],\n","                                                                 seed=42)\n","\n","\n","\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n","tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","converter._experimental_lower_tensor_list_ops = False\n","\n","tflite_quant_model = converter.convert()\n","\n","\n","with open('converted_model_BiLSTM_Quantizzazione_Float16.tflite', 'wb') as f:\n","    f.write(tflite_quant_model)\n","\n","model_quantized_size = os.path.getsize('converted_model_BiLSTM_Quantizzazione_Float16.tflite')\n","model_quantized_size_mb = model_quantized_size / (1024 * 1024)\n","print(f\"Quantizzazione completata e modello salvato come 'onverted_model_BiLSTM_Quantizzazione_Float16.tflite'  Peso modello: {model_quantized_size_mb} MB\")\n"],"metadata":{"id":"XcEHCASmyaeB"},"execution_count":null,"outputs":[]}]}