{"cells":[{"cell_type":"markdown","metadata":{"id":"oeGSLl43gNG3"},"source":["# Quantizzazione AWARE TRAINING per i modelli ConvLSTM e BiLSTM\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LVerPgcditUj"},"source":["## 1 Operazioni preliminari e Data Pre-Processing (DPP)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tCVXQsZve1P3"},"outputs":[],"source":["# Monto drive Google\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2_-Hx-iyKeNG"},"outputs":[],"source":["# Funzioni per DPP e utilit√† (chunk count, video preprocessing, feature computation, ...)\n","\n","import os\n","import cv2\n","import numpy as np\n","\n","def count_chunks(videoBasePath):\n","    \"\"\"Counts the 16 frames lenght chunks available in a dataset organized in Violence and NonViolence,\n","    cam1 and cam2 folders, placed at videoBasePath.\n","\n","    Parameters\n","    ----------\n","    videoBasePath : str\n","                    Base path of the dataset\n","\n","    Returns\n","    -------\n","    cnt : int\n","          number of 16 frames lenght chunks in the dataset\n","    \"\"\"\n","\n","    folders = ['Violence', 'NonViolence']\n","    cams = ['cam1', 'cam2']\n","    cnt = 0\n","\n","    for folder in folders:\n","        for camName in cams:\n","            path = os.path.join(videoBasePath, folder, camName)\n","\n","            videofiles = os.listdir(path)\n","            for videofile in videofiles:\n","                filePath = os.path.join(path, videofile)\n","                video = cv2.VideoCapture(filePath)\n","                numframes = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n","                fps = int(video.get(cv2.CAP_PROP_FPS))\n","                chunks = numframes//16\n","                cnt += chunks\n","\n","\n","    return cnt\n","\n","def preprocessVideos(videoBasePath, mainDir, featureBasePath, verbose=True):\n","    \"\"\"Preproccess all the videos.\n","\n","    It extracts samples from the videos organised in violent and non-violent, cam1 and cam2 folders.\n","    The samples and the labels are store on two memmap numpy arrays, called samples.mmap and labels.mmap, at \"featureBasePath\".\n","    The numpy array with samples has shape (Chunk #, 16, 224, 224, 3), the labels array has shape (Chunk # 16, 224, 224, 3).\n","    For the AIRTLab dataset the number of chunks is 3537.\n","\n","    Parameters\n","    ----------\n","    videoBasePath : str\n","                    Pathname to the base of the video repository, which contains two directories,\n","                    violent and non-violent, which are divided into cam1 and cam2.\n","    mainDir: str\n","             Pathaname to store the files with sample filenames and labels.\n","    featureBasePath : str\n","                      it is the pathname of a base where the numpy arrays have to be saved.\n","    verbose : bool\n","              if True print debug logs (default True)\n","\n","    \"\"\"\n","\n","    folders = ['Violence', 'NonViolence']\n","    cams = ['cam1', 'cam2']\n","    total_chunks = count_chunks(videoBasePath)\n","    fileNames = []\n","    npLabels = np.zeros(total_chunks)\n","    cnt = 0\n","\n","    for folder in folders:\n","        for camName in cams:\n","            path = os.path.join(videoBasePath, folder, camName)\n","\n","            videofiles = os.listdir(path)\n","            for videofile in videofiles:\n","                filePath = os.path.join(path, videofile)\n","                video = cv2.VideoCapture(filePath)\n","                numframes = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n","                fps = int(video.get(cv2.CAP_PROP_FPS))\n","                chunks = numframes//16\n","                if verbose:\n","                    print(filePath)\n","                    print(\"*** [Video Info] Number of frames: {} - fps: {} - chunks: {}\".format(numframes, fps, chunks))\n","                vid = []\n","                videoFrames = []\n","                while True:\n","                    ret, img = video.read()\n","                    if not ret:\n","                        break\n","                    videoFrames.append(cv2.resize(img, (224, 224)))\n","                vid = np.array(videoFrames, dtype=np.float32)\n","                filename = os.path.splitext(videofile)[0]\n","                chunk_cnt = 0\n","                for i in range(chunks):\n","                    X = vid[i*16:i*16+16]\n","                    chunk_cnt += 1\n","                    filename = folder + '_' + camName + '_' + videofile + '_chunk_' + str(chunk_cnt) + '.npy'\n","                    fileNames.append(filename)\n","                    savepath = os.path.join(featureBasePath, filename)\n","                    np.save(savepath, np.array(X, dtype=np.float32))\n","                    if folder == 'Violence':\n","                        npLabels[cnt] = np.int8(1)\n","                    else:\n","                        npLabels[cnt] = np.int8(0)\n","                    cnt += 1\n","\n","    fileNamesNp = os.path.join(mainDir, 'filenames.npy')\n","    np.save(fileNamesNp, fileNames)\n","\n","    labelsNp = os.path.join(mainDir, 'labels.npy')\n","    np.save(labelsNp, npLabels)\n","\n","    if verbose:\n","        print(\"** Labels **\")\n","        print(npLabels.shape)\n","        print('\\n****\\n')\n","        print(\"** Samples **\")\n","        print(len(fileNames))\n","        print('\\n****\\n')\n","\n","    del fileNames\n","    del npLabels"]},{"cell_type":"markdown","metadata":{"id":"33T200nDn2sq"},"source":["## 2 Esecuzione Data Pre-Processing (DPP)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5tK07PDR6OC"},"outputs":[],"source":["# Cartelle per memorizzare i campioni (le sottosequenze di 16 frame) da usare\n","# per training, validation e test.\n","\n","paths = [\"/airtlabDataset\", \"/airtlabDataset/features\", \"/airtlabDataset/results\"]\n","for path in paths:\n","  if not os.path.isdir(path):\n","    os.mkdir(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8YrDkTN2Kdj"},"outputs":[],"source":["preprocessVideos('/content/gdrive/My Drive/Dataset/AirtLab-Dataset', '/airtlabDataset', '/airtlabDataset/features', True)"]},{"cell_type":"markdown","metadata":{"id":"A1i9eyYf53DF"},"source":["## 3 Esperimento\n"]},{"cell_type":"code","source":["!pip install tensorflow-model-optimization"],"metadata":{"id":"u1j0juiF3y8J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow_model_optimization as tfmot\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","import keras\n","\n","\n","def prepare_model_to_QAT(model):\n","    quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n","    annotated_layers = []\n","\n","    # Itera attraverso i layer del modello\n","    for layer in model.layers:\n","        # Layer Incompatibili\n","        if  isinstance(layer, ( keras.layers.Bidirectional, keras.layers.TimeDistributed, keras.layers.Dropout , keras.layers.Dense)):\n","            print(f\"Layer {layer} incompatibile per la QAT\")\n","            annotated_layers.append(layer)\n","\n","        # LayerCompatibili\n","        else:\n","            annotated_layers.append(quantize_annotate_layer(layer))\n","\n","    # Costruisce un nuovo modello con i layer annotati\n","    q_aware_model = tf.keras.Sequential(annotated_layers)\n","\n","    q_aware_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    return q_aware_model\n","\n","def prepare_model_to_QAT_ConvLstm(model):\n","    quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n","    annotated_layers = []\n","\n","    # Itera attraverso i layer del modello\n","    for layer in model.layers:\n","        # Layer Incompatibili\n","        if  isinstance(layer, ( keras.layers.TimeDistributed, keras.layers.ConvLSTM2D, keras.layers.Flatten, keras.layers.Dropout, keras.layers.Dense)):\n","            print(f\"Layer {layer} incompatibile per la QAT\")\n","            annotated_layers.append(layer)\n","\n","        # LayerCompatibili\n","        else:\n","            annotated_layers.append(quantize_annotate_layer(layer))\n","\n","    # Costruisce un nuovo modello con i layer annotati\n","    q_aware_model = tf.keras.Sequential(annotated_layers)\n","\n","    q_aware_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    return q_aware_model\n","\n","def save_model_as_tflite(model, filename):\n","    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","\n","    converter.target_spec.supported_ops = [\n","      tf.lite.OpsSet.TFLITE_BUILTINS,\n","      tf.lite.OpsSet.SELECT_TF_OPS\n","    ]\n","\n","    # Disabilita le operazioni non supportate\n","    converter._experimental_lower_tensor_list_ops = False\n","\n","    # Conversione del modello TensorFlow in formato TFLite\n","    tflite_model = converter.convert()\n","\n","\n","    with open(filename, 'wb') as f:\n","        f.write(tflite_model)\n","\n"],"metadata":{"id":"Vc_lxT61vLvT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rGdFcPb7fYz4"},"outputs":[],"source":["# definitions of two end-to-end models + definitions of experiments\n","\n","from tensorflow.keras import layers\n","import pandas as pd\n","import numpy as np\n","import sklearn\n","from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n","from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, classification_report\n","from keras.callbacks import EarlyStopping\n","import matplotlib.pylab as plt\n","import os\n","from keras.models import Sequential, Model\n","from keras.layers import Input, Dense, Dropout, Flatten, ConvLSTM2D, TimeDistributed, Bidirectional, LSTM\n","from keras.utils import Sequence\n","\n","class DataGen(Sequence):\n","    \"\"\" A sequence of data for training/test/validation, loaded from memory\n","    batch by batch. Extends the tensorflow.keras.utils.Sequence: https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n","\n","    Attributes\n","    ----------\n","    base_path : str\n","                path to the folder including the samples.\n","    filenames : list<str>\n","                list of sample filenames.\n","    labels : list<str>\n","             list of sample labels.\n","    batch_size : int\n","                 batch size to load samples\n","\n","    \"\"\"\n","\n","    def __init__(self, base_path, filenames, labels, batch_size, Preprocess_input):\n","        self.base_path = base_path\n","        self.filenames = filenames\n","        self.labels = labels\n","        self.batch_size = batch_size\n","        self.Preprocess_input = Preprocess_input\n","\n","    def __len__(self):\n","        return (np.ceil(len(self.filenames) / float(self.batch_size))).astype(int)\n","\n","    def __getitem__(self, idx):\n","        batch_x = self.filenames[idx * self.batch_size: (idx + 1) * self.batch_size]\n","        batch_y = self.labels[idx * self.batch_size: (idx + 1) * self.batch_size]\n","\n","        return np.array([self.Preprocess_input(np.load(os.path.join(self.base_path, file_name))) for file_name in batch_x]), np.array(batch_y)\n","\n","def GetPretrainedModel(ModelConstructor, input_shape=(224, 224, 3), print_summary=True):\n","    \"\"\" Builds the VGG16 2D CNN with the Imagenet weights, freezing all layers except layers_to_finetune\n","\n","    Parameters\n","    ----------\n","    ModelConstructor : Callable[[bool], [str], [tuple], Sequential]\n","                       Function that download the pretrained model, i.e. one of the Keras applications:\n","                       https://keras.io/api/applications/\n","                       The arguments are include_top, weights, and input_shape.\n","    input_shape : tuple\n","                  The input shape for the pretrained model.\n","    print_summary : bool\n","                    If True prints the model summary.\n","\n","    Returns\n","    -------\n","    model : Sequential\n","          The instantiated model.\n","    \"\"\"\n","\n","    model = ModelConstructor(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n","\n","    for layer in model.layers:\n","        layer.trainable = False\n","\n","\n","    return model\n","\n","\n","def getLSTMModel(getConvModel, ModelConstructor, pretrained_input_shape=(224, 224, 3), verbose=True):\n","    \"\"\"Creates the BiLSTM + fully connected layers end-to-end model object\n","    with the sequential API: https://keras.io/models/sequential/\n","\n","    Parameters\n","    ----------\n","    getConvModel : Callable[Callable[[bool], [str], [tuple], Sequential], [tuple], [bool], Sequential]\n","                Function that instantiates the pretrained Convolutional model\n","                to be applied in a time distributed fashion.\n","    ModelConstructor : Callable[[bool], [str], [tuple], Sequential]\n","                       Function that download the pretrained model, i.e. one of the Keras applications:\n","                       https://keras.io/api/applications/\n","                       The arguments are include_top, weights, and input_shape.\n","    input_shape : tuple\n","                  The input shape for the pretrained model.\n","    verbose : bool\n","              if True prints the model summary (default True)\n","\n","    Returns\n","    -------\n","    model : Sequential\n","            The instantiated model\n","    \"\"\"\n","    model = Sequential()\n","    model.add(TimeDistributed(getConvModel(ModelConstructor, pretrained_input_shape, verbose), input_shape=(16, 224, 224, 3)))\n","\n","    model.add(TimeDistributed(Flatten()))\n","    model.add(Bidirectional(LSTM(units=128, return_sequences=False)))\n","\n","    model.add(Dropout(0.5))\n","    model.add(Dense(128, activation='relu'))\n","\n","    model.add(Dropout(0.5))\n","    model.add(Dense(1, activation='sigmoid'))\n","    if verbose:\n","        model.summary()\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    return model\n","\n","def getConvLSTMModel(getConvModel, ModelConstructor, pretrained_input_shape=(224, 224, 3), verbose=True):\n","    \"\"\"Creates the BiLSTM + fully connected layers end-to-end model object\n","    with the sequential API: https://keras.io/models/sequential/\n","\n","    Parameters\n","    ----------\n","    getConvModel : Callable[Callable[[bool], [str], [tuple], Sequential], [tuple], [bool], Sequential]\n","                Function that instantiates the pretrained Convolutional model\n","                to be applied in a time distributed fashion.\n","    ModelConstructor : Callable[[bool], [str], [tuple], Sequential]\n","                       Function that download the pretrained model, i.e. one of the Keras applications:\n","                       https://keras.io/api/applications/\n","                       The arguments are include_top, weights, and input_shape.\n","    input_shape : tuple\n","                  The input shape for the pretrained model.\n","    verbose : bool\n","              if True prints the model summary (default True)\n","\n","    Returns\n","    -------\n","    model : Sequential\n","            The instantiated model\n","    \"\"\"\n","    model = Sequential()\n","    model.add(TimeDistributed(getConvModel(ModelConstructor, pretrained_input_shape, verbose), input_shape=(16, 224, 224, 3)))\n","\n","    model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3)))\n","\n","    model.add(Flatten())\n","    model.add(Dropout(0.5))\n","    model.add(Dense(256, activation='relu'))\n","\n","    model.add(Dropout(0.5))\n","    model.add(Dense(1, activation='sigmoid'))\n","    if verbose:\n","        model.summary()\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    return model\n","\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\n","from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n","\n","def runEndToEndExperiment(getLSTMModel, getConvModel, ModelConstructor, pretrained_input_shape, Preprocess_input, batchSize, datasetBasePath, npyBasePath, featuresPath, samplesMMapName, lablesMMapName, endToEndModelName, rState, savePath, isConvLstm=False):\n","    \"\"\"Runs the tests with end to end models.\n","\n","    Parameters\n","    ----------\n","    getLSTMModel : Callable[[Callable[Callable[[bool], [str], [tuple], Sequential],\n","                   [tuple], [bool], Sequential]], [Callable[[bool], [str], [tuple],\n","                   Sequential]], [tuple], [bool], Sequential]\n","                   Function that instantiates the model to be tested. The parameters\n","                   are a function that returns the Convolutional model to be tested\n","                   in a time distributed fashion, and a boolean for verbose output\n","    getConvModel : Callable[Callable[[bool], [str], [tuple], Sequential], [tuple], [bool], Sequential]\n","                Function that instantiates the pretrained Convolutional model\n","                to be applied in a time distributed fashion.\n","    ModelConstructor : Callable[[bool], [str], [tuple], Sequential]\n","                       Function that download the pretrained model, i.e. one of the Keras applications:\n","                       https://keras.io/api/applications/\n","                       The arguments are include_top, weights, and input_shape.\n","    input_shape : tuple\n","                  The input shape for the pretrained model.\n","    batchSize : int\n","                Batch size to be used for training and testing\n","    datasetBasePath : str\n","                      Pathname to the base of the feature files repository,\n","                      which contains two directories, violent and non-violent,\n","                      which are divided into cam1 and cam2.\n","    npyBasePath : str\n","                  Pathname where the files with sample filenames and labels are\n","                  stored.\n","    featuresPath : str\n","                  Folder containing the actual files with the samples.\n","    samplesMMapName : str\n","                      Name of the file storing the list with sample filenames.\n","    lablesMMapName : str\n","                     Name of the file storing the list of sample labels.\n","    endToEndModelName : str\n","                        Model name to be used in the AUC-ROC plot.\n","    rState : int, RandomState instance or None\n","             Controls the randomness of the training and testing indices produced.\n","             Pass an int for reproducible output across multiple function calls.\n","    savePath : str\n","               Path to the directory where the model and weights will be saved.\n","    \"\"\"\n","\n","    chunk_number = count_chunks(datasetBasePath)\n","    X = np.load(os.path.join(npyBasePath, samplesMMapName))\n","    y = np.load(os.path.join(npyBasePath, lablesMMapName))\n","\n","    nsplits = 5\n","    cv = StratifiedShuffleSplit(n_splits=nsplits, train_size=0.8, random_state=rState)\n","\n","    tprs = []\n","    aucs = []\n","    scores = []\n","    sens = np.zeros(shape=(nsplits))\n","    specs = np.zeros(shape=(nsplits))\n","    f1Scores = np.zeros(shape=(nsplits))\n","    mean_fpr = np.linspace(0, 1, 100)\n","    plt.figure(num=1, figsize=(10, 10))\n","    i = 1\n","\n","    for train, test in cv.split(X, y):\n","        X_train, X_val, y_train, y_val = train_test_split(X[train][:], y[train], test_size=0.125, random_state=rState)\n","\n","        filepath = os.path.join(npyBasePath, featuresPath)\n","\n","        training_batch_generator = DataGen(filepath, X_train, y_train, batchSize, Preprocess_input)\n","        validation_batch_generator = DataGen(filepath, X_val, y_val, batchSize, Preprocess_input)\n","        test_batch_generator = DataGen(filepath, X[test][:], y[test], batchSize, Preprocess_input)\n","\n","        model = getLSTMModel(getConvModel, ModelConstructor, pretrained_input_shape, i == 1)\n","\n","        if isConvLstm:\n","          model = prepare_model_to_QAT_ConvLstm(model)\n","        else:\n","          model = prepare_model_to_QAT(model)\n","\n","        # Define the ModelCheckpoint callback to save the best weights\n","        checkpoint_path = f\"best_model_fold_{i}_epoch_{{epoch:02d}}.h5\"\n","        mc = ModelCheckpoint(checkpoint_path, monitor='val_loss', mode='min', save_best_only=True, save_weights_only=True, verbose=1)\n","\n","        es = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1, restore_best_weights=True)\n","\n","        history = model.fit(x=training_batch_generator, validation_data=validation_batch_generator, epochs=50, verbose=1, callbacks=[es, mc])\n","\n","        # Convert and save the best model of each epoch to TFLite\n","        for epoch in range(1, len(history.history['loss']) + 1):\n","            best_model_path = checkpoint_path.format(epoch=epoch)\n","            if os.path.exists(best_model_path):\n","                model.load_weights(best_model_path)\n","                save_model_as_tflite(model, f\"best_model_fold_{i}_epoch_{epoch:02d}.tflite\")\n","\n","        # Save the final model of the current fold to TFLite\n","        final_model_path = os.path.join(savePath, f'final_model_fold_{i}.tflite')\n","        try:\n","            save_model_as_tflite(model, final_model_path)\n","            print(f\"Salvataggio ultima epoca split {i} effettuato\")\n","        except Exception as e:\n","            print(f\"Errore durante il salvataggio del modello TFLite per l'ultima epoca del fold {i}: {e}\")\n","\n","\n","        print(\"Computing scores...\")\n","        evaluation = model.evaluate(x=test_batch_generator)\n","        scores.append(evaluation)\n","        print(\"Computing probs...\")\n","        probas = model.predict(x=test_batch_generator, verbose=1).ravel()\n","\n","        fpr, tpr, thresholds = roc_curve(y[test], probas)\n","        tprs.append(np.interp(mean_fpr, fpr, tpr))\n","        roc_auc = auc(fpr, tpr)\n","        aucs.append(roc_auc)\n","        plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC split %d (AUC = %0.4f)' % (i, roc_auc))\n","\n","        y_pred = np.round(probas)\n","        report = classification_report(y[test], y_pred, target_names=['non-violent', 'violent'], output_dict=True)\n","        sens[i - 1] = report['violent']['recall']\n","        specs[i - 1] = report['non-violent']['recall']\n","        f1Scores[i - 1] = report['violent']['f1-score']\n","\n","        print('confusion matrix split ' + str(i))\n","        print(confusion_matrix(y[test], y_pred))\n","        print(classification_report(y[test], y_pred, target_names=['non-violent', 'violent']))\n","        print('Loss: ' + str(evaluation[0]))\n","        print('Accuracy: ' + str(evaluation[1]))\n","        print('\\n')\n","\n","        i = i + 1\n","\n","        del report\n","        del model\n","\n","    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n","\n","    mean_tpr = np.mean(tprs, axis=0)\n","    mean_auc = auc(mean_fpr, mean_tpr)\n","    std_auc = np.std(aucs)\n","    plt.plot(mean_fpr, mean_tpr, color='b', label=r'Mean ROC (AUC = %0.4f $\\pm$ %0.4f)' % (mean_auc, std_auc), lw=2, alpha=.8)\n","\n","    std_tpr = np.std(tprs, axis=0)\n","    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n","    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n","    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label=r'$\\pm$ 1 std. dev.')\n","\n","    plt.xlim([-0.01, 1.01])\n","    plt.ylim([-0.01, 1.01])\n","    plt.xlabel('False Positive Rate', fontsize=18)\n","    plt.ylabel('True Positive Rate', fontsize=18)\n","    plt.title('Cross-Validation ROC of ' + endToEndModelName + ' model', fontsize=18)\n","    plt.legend(loc=\"lower right\", prop={'size': 15})\n","\n","    np_scores = np.array(scores)\n","    losses = np_scores[:, 0:1]\n","    accuracies = np_scores[:, 1:2]\n","    print('Losses')\n","    print(losses)\n","    print('Accuracies')\n","    print(accuracies)\n","    print('Sensitivities')\n","    print(sens)\n","    print('Specificities')\n","    print(specs)\n","    print('F1-scores')\n","    print(f1Scores)\n","    print(\"Avg loss: {0} +/- {1}\".format(np.mean(losses), np.std(losses)))\n","    print(\"Avg accuracy: {0} +/- {1}\".format(np.mean(accuracies), np.std(accuracies)))\n","    print(\"Avg sensitivity: {0} +/- {1}\".format(np.mean(sens), np.std(sens)))\n","    print(\"Avg specificity: {0} +/- {1}\".format(np.mean(specs), np.std(specs)))\n","    print(\"Avg f1-score: {0} +/- {1}\".format(np.mean(f1Scores), np.std(f1Scores)))\n","\n","    plt.savefig(endToEndModelName.replace('+', '') + '.pdf')\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YpcrpnbeS9xJ"},"outputs":[],"source":["# ConvLSTM\n","\n","from tensorflow.keras.applications import MobileNetV3Small\n","from tensorflow.keras.applications.mobilenet_v3 import preprocess_input as mobilenet_v3_preprocess_input\n","from tensorflow.keras import layers\n","\n","\n","runEndToEndExperiment(\n","    getLSTMModel,\n","    GetPretrainedModel,\n","    MobileNetV3Small,\n","    (224, 224, 3),\n","    mobilenet_v3_preprocess_input,\n","    8,\n","    '/content/gdrive/My Drive/Dataset/AirtLab-Dataset',\n","    '/airtlabDataset',\n","    'features',\n","    'filenames.npy',\n","    'labels.npy',\n","    'MobileNetV3Small + BiLSTM',\n","    42,\n","    '/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/Quantizzazione Aware Training/BiLSTM'\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJeCupPlcHtE"},"outputs":[],"source":["# BiLSTM\n","\n","from tensorflow.keras.applications import MobileNetV3Small\n","from tensorflow.keras.applications.mobilenet_v3 import preprocess_input as mobilenet_v3_preprocess_input\n","from tensorflow.keras import layers\n","\n","\n","\n","runEndToEndExperiment(\n","    getConvLSTMModel,\n","    GetPretrainedModel,\n","    MobileNetV3Small,\n","    (224, 224, 3),\n","    mobilenet_v3_preprocess_input,\n","    8,\n","    '/content/gdrive/My Drive/Dataset/AirtLab-Dataset',\n","    '/airtlabDataset',\n","    'features',\n","    'filenames.npy',\n","    'labels.npy',\n","    'MobileNetV3Small + ConvLSTM',\n","    42,\n","    '/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/Quantizzazione Aware Training/ConvLSTm',\n","    True\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFnAGLn85_WM"},"outputs":[],"source":["from keras import __version__\n","from keras import backend as K\n","import sklearn\n","\n","print('Using Keras version:', __version__, 'backend:', K.backend())\n","\n","if K.backend() == \"tensorflow\":\n","    import tensorflow as tf\n","    device_name = tf.test.gpu_device_name()\n","    if device_name == '':\n","        device_name = \"None\"\n","    print('Using TensorFlow version:', tf.__version__, ', GPU:', device_name)\n","\n","print('The scikit-learn version is {}.'.format(sklearn.__version__))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"alLhTYiB8TQh"},"outputs":[],"source":["!nvidia-smi"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"collapsed_sections":["LVerPgcditUj","33T200nDn2sq"]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}