{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["o7F7S9l88MMt","UtsWhqSqnp4X","yUq9tJ0usVzM"],"authorship_tag":"ABX9TyMHvffQlYlkM4n+ihFxiJSe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Monto il drive Google\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"metadata":{"id":"yFSH8aRPvhoz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install codecarbon"],"metadata":{"id":"68HIwPInp4DA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MobileNet-V3 Small + ConvLSTM (soluzione QPT Int con fallback FLoat e Pruning)\n","\n","Verranno effettuate in ordine le seguenti operazioni:\n","\n","\n","1.   Caricamento del modello \"prunato\" al 50 %\n","2.   Quantizzazione intera 8 bit a fallback Float del modello prunato.\n","\n","I risultati, sono i seguenti:\n","\n","\n","\n","*   Peso modello: circa  3.3 MB\n","*   Accuratezza modello: 80  %\n","*   Auc: circa 85.5 %\n","*   Tempo inferenza medio per batch: 0.35 s\n","*   Consumo Energetico medio per 102 secondi di video:  0.0018 kW/h\n","\n","\n"],"metadata":{"id":"ZpejtYmbu8lR"}},{"cell_type":"code","source":["# Carico il modello prunato (non in formato tflite)\n","\n","from tensorflow.keras.models import load_model\n","\n","\n","pruned_model= load_model('/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/Pruning/50_perc/pruned_model_ConvLSTM.h5')\n","pruned_model.summary()"],"metadata":{"id":"f_SYN4TH1S0s"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wMe8UJt8uvaZ"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import cv2\n","import os\n","\n","# Dimensioni del batch di input alla rete\n","batch_size = 16\n","height = 224\n","width = 224\n","channels = 3\n","\n","# Funzione per estrarre frame da un video e creare i batch\n","def generate_video_batches(video_paths, seed=None):\n","    batches = []\n","    for video_path in video_paths:\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame = cv2.resize(frame, (width, height))\n","            frames.append(frame)\n","            if len(frames) == batch_size:\n","                batches.append(frames)\n","                frames = []\n","        cap.release()\n","\n","    for batch in batches:\n","        yield [np.array(batch, dtype=np.float32)]\n","\n","# Imposta il convertitore\n","converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.representative_dataset = lambda: generate_video_batches(['/content/gdrive/MyDrive/Dataset/AirtLab-Dataset/Violence/cam1/1.mp4'],\n","                                                                 seed=42)\n","\n","# Abilita Select TF ops e disabilita experimental lowering per tensor list ops\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","converter._experimental_lower_tensor_list_ops = False\n","\n","tflite_quant_model = converter.convert()\n","\n","with open('model_quantizzato_prunato_50.tflite', 'wb') as f:\n","    f.write(tflite_quant_model)\n","\n","model_quantized_size = os.path.getsize('model_quantizzato_prunato_50.tflite')\n","model_quantized_size_mb = model_quantized_size / (1024 * 1024)\n","print(f\"Quantizzazione completata e modello salvato come 'model_quantizzato_prunato_50.tflite'  Peso modello: {model_quantized_size_mb} MB\")\n","\n"]},{"cell_type":"code","source":["# Funzioni per pre processamento dei video e inferenza\n","\n","import os\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","import time\n","\n","def load_tflite_model(model_path):\n","    interpreter = tf.lite.Interpreter(model_path=model_path)\n","    interpreter.allocate_tensors()\n","    return interpreter\n","\n","def preprocess_video(video_path, input_shape, batch_size):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, (input_shape[1], input_shape[0]))\n","        frame = frame.astype('float32')\n","        frames.append(frame)\n","\n","    cap.release()\n","    frames = np.array(frames)\n","\n","    if len(frames) >= batch_size:\n","        num_batches = len(frames) // batch_size\n","        frames = frames[:num_batches * batch_size]\n","        batches = np.split(frames, num_batches)\n","    else:\n","        batches = []\n","\n","    return batches\n","\n","def run_inference(interpreter, input_data):\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","    interpreter.set_tensor(input_details[0]['index'], input_data)\n","\n","    # Calcola il tempo di inferenza\n","    start_time = time.time()\n","    interpreter.invoke()\n","    end_time = time.time()\n","    inference_time = end_time - start_time\n","\n","    output_data = interpreter.get_tensor(output_details[0]['index'])\n","\n","    return output_data, inference_time\n","\n","def calculate_metrics(y_true, y_pred, y_probs):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    specificity = recall_score(y_true, y_pred, pos_label=0)\n","    fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n","    roc_auc = auc(fpr, tpr)\n","\n","    return accuracy, precision, recall, specificity, roc_auc, fpr, tpr\n","\n","def plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve'):\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='b', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='r', linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(title)\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","def load_videos_from_folder(folder_path, label, input_shape, batch_size):\n","    videos = []\n","    labels = []\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".mp4\") or filename.endswith(\".avi\"):\n","            video_path = os.path.join(folder_path, filename)\n","            video_batches = preprocess_video(video_path, input_shape, batch_size)\n","            videos.extend(video_batches)\n","            labels.extend([label] * len(video_batches))\n","    return videos, labels\n","\n","\n","from codecarbon import track_emissions\n","@track_emissions(project_name=\"MV3_Small_Inference_ConvLSTM_QPT_intFallbackFLoat_Pruned\")\n","def inferenceV3_ConvLSTM():\n","    model_path = '/content/model_quantizzato_prunato_50.tflite'\n","    violence_path = '/content/gdrive/MyDrive/VideoInferenza/Violence'\n","    nonviolence_path = '/content/gdrive/MyDrive/VideoInferenza/NonViolence'\n","    input_shape = (224, 224)\n","    batch_size = 16\n","\n","    interpreter = load_tflite_model(model_path)\n","\n","    violence_videos, violence_labels = load_videos_from_folder(violence_path, 1, input_shape, batch_size)\n","    nonviolence_videos, nonviolence_labels = load_videos_from_folder(nonviolence_path, 0, input_shape, batch_size)\n","\n","    all_videos = violence_videos + nonviolence_videos\n","    all_labels = violence_labels + nonviolence_labels\n","\n","    all_videos = np.array([np.expand_dims(video, axis=0) for video in all_videos])\n","    all_labels = np.array(all_labels)\n","\n","    total_inference_time = 0\n","    y_probs = []\n","    for video in all_videos:\n","        output_data, inference_time = run_inference(interpreter, video)\n","        total_inference_time += inference_time\n","        y_probs.append(output_data.ravel()[0])\n","\n","    y_probs = np.array(y_probs)\n","    y_pred = np.round(y_probs)\n","\n","    # Calculate metrics\n","    accuracy, precision, recall, specificity, roc_auc, fpr, tpr = calculate_metrics(all_labels, y_pred, y_probs)\n","\n","    # Calculate average inference time\n","    average_inference_time = total_inference_time / len(all_videos)\n","\n","    # Print metrics\n","    print(f'Accuracy: {accuracy:.4f}')\n","    print(f'Precision: {precision:.4f}')\n","    print(f'Recall: {recall:.4f}')\n","    print(f'Specificity: {specificity:.4f}')\n","    print(f'ROC AUC: {roc_auc:.4f}')\n","    print(f'Average Inference Time: {average_inference_time:.4f} seconds')\n","\n","    # Print classification report and confusion matrix\n","    print('Classification Report:')\n","    print(classification_report(all_labels, y_pred))\n","\n","    print('Confusion Matrix:')\n","    print(confusion_matrix(all_labels, y_pred))\n","\n","    # Plot ROC curve\n","    plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve for tf_model_mv3_convLStm_quantizzato_int_fallbackFloat_prunato')\n","\n","if __name__ == '__main__':\n","    inferenceV3_ConvLSTM()\n"],"metadata":{"id":"MOJbbC9-23oF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MobileNet-V3 Small + BiLSTM (soluzione QPT Int con fallback FLoat e Pruning)\n","\n","Verranno effettuate in ordine le seguenti operazioni:\n","\n","\n","1.   Caricamento del modello \"prunato\" al 50 %\n","2.   Quantizzazione intera 8 bit a fallback Float del modello prunato.\n","\n","I risultati, sono i seguenti:\n","\n","\n","\n","*   Peso modello: circa  28.9 MB\n","*   Accuratezza modello: 74.5 %\n","*   Auc: circa 84.89 %\n","*   Tempo inferenza medio per batch: 0.415 s\n","*   Consumo Energetico medio per 102 secondi di video:  0.001784 kW/h\n"],"metadata":{"id":"o7F7S9l88MMt"}},{"cell_type":"code","source":["# Carico il modello prunato (non in formato tflite)\n","from tensorflow.keras.models import load_model\n","\n","\n","pruned_model= load_model('/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/Pruning/50_perc/pruned_model_BiLSTM.h5')\n","pruned_model.summary()"],"metadata":{"id":"snvqjLs286bL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import cv2\n","import os\n","\n","# Dimensione dei batch di input alla rete\n","batch_size = 16\n","height = 224\n","width = 224\n","channels = 3\n","\n","# Funzione per estrarre frame da un video e creare i batch\n","def generate_video_batches(video_paths, seed=None):\n","    batches = []\n","    for video_path in video_paths:\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame = cv2.resize(frame, (width, height))\n","            frames.append(frame)\n","            if len(frames) == batch_size:\n","                batches.append(frames)\n","                frames = []\n","        cap.release()\n","\n","\n","    for batch in batches:\n","        yield [np.array(batch, dtype=np.float32)]\n","\n","# Imposta il convertitore\n","converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.representative_dataset = lambda: generate_video_batches(['/content/gdrive/MyDrive/Dataset/AirtLab-Dataset/Violence/cam1/1.mp4'],\n","                                                                 seed=42)  # Specifica il seme qui\n","\n","# Abilita Select TF ops e disabilita experimental lowering per tensor list ops\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","converter._experimental_lower_tensor_list_ops = False\n","\n","\n","tflite_quant_model = converter.convert()\n","\n","\n","with open('model_quantizzato_int_fallbackFloat_prunato_50_BiLSTM.tflite', 'wb') as f:\n","    f.write(tflite_quant_model)\n","\n","model_quantized_size = os.path.getsize('model_quantizzato_int_fallbackFloat_prunato_50_BiLSTM.tflite')\n","model_quantized_size_mb = model_quantized_size / (1024 * 1024)\n","print(f\"Quantizzazione completata e modello salvato come 'model_quantizzato_prunato_50_BiLSTM.tflite'  Peso modello: {model_quantized_size_mb} MB\")\n","\n"],"metadata":{"id":"yfQsafVt952a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funzioni per il pre processamento dei video e l'inferenza\n","\n","import os\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","import time\n","\n","def load_tflite_model(model_path):\n","    interpreter = tf.lite.Interpreter(model_path=model_path)\n","    interpreter.allocate_tensors()\n","    return interpreter\n","\n","def preprocess_video(video_path, input_shape, batch_size):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, (input_shape[1], input_shape[0]))\n","        frame = frame.astype('float32')\n","        frames.append(frame)\n","\n","    cap.release()\n","    frames = np.array(frames)\n","\n","    if len(frames) >= batch_size:\n","        num_batches = len(frames) // batch_size\n","        frames = frames[:num_batches * batch_size]\n","        batches = np.split(frames, num_batches)\n","    else:\n","        batches = []\n","\n","    return batches\n","\n","def run_inference(interpreter, input_data):\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","    interpreter.set_tensor(input_details[0]['index'], input_data)\n","\n","    # Calcola il tempo di inferenza\n","    start_time = time.time()\n","    interpreter.invoke()\n","    end_time = time.time()\n","    inference_time = end_time - start_time\n","\n","    output_data = interpreter.get_tensor(output_details[0]['index'])\n","\n","    return output_data, inference_time\n","\n","def calculate_metrics(y_true, y_pred, y_probs):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    specificity = recall_score(y_true, y_pred, pos_label=0)\n","    fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n","    roc_auc = auc(fpr, tpr)\n","\n","    return accuracy, precision, recall, specificity, roc_auc, fpr, tpr\n","\n","def plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve'):\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='b', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='r', linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(title)\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","def load_videos_from_folder(folder_path, label, input_shape, batch_size):\n","    videos = []\n","    labels = []\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".mp4\") or filename.endswith(\".avi\"):\n","            video_path = os.path.join(folder_path, filename)\n","            video_batches = preprocess_video(video_path, input_shape, batch_size)\n","            videos.extend(video_batches)\n","            labels.extend([label] * len(video_batches))\n","    return videos, labels\n","\n","\n","from codecarbon import track_emissions\n","@track_emissions(project_name=\"MV3_Small_Inference_BiLSTM_quantizzato_Int_prunato\")\n","def inferenceV3_BiLSTM():\n","    model_path = 'model_quantizzato_int_fallbackFloat_prunato_50_BiLSTM.tflite'\n","    violence_path = '/content/gdrive/MyDrive/VideoInferenza/Violence'\n","    nonviolence_path = '/content/gdrive/MyDrive/VideoInferenza/NonViolence'\n","    input_shape = (224, 224)\n","    batch_size = 16\n","\n","    interpreter = load_tflite_model(model_path)\n","\n","    violence_videos, violence_labels = load_videos_from_folder(violence_path, 1, input_shape, batch_size)\n","    nonviolence_videos, nonviolence_labels = load_videos_from_folder(nonviolence_path, 0, input_shape, batch_size)\n","\n","    all_videos = violence_videos + nonviolence_videos\n","    all_labels = violence_labels + nonviolence_labels\n","\n","    all_videos = np.array([np.expand_dims(video, axis=0) for video in all_videos])\n","    all_labels = np.array(all_labels)\n","\n","    total_inference_time = 0\n","    y_probs = []\n","    for video in all_videos:\n","        output_data, inference_time = run_inference(interpreter, video)\n","        total_inference_time += inference_time\n","        y_probs.append(output_data.ravel()[0])\n","\n","    y_probs = np.array(y_probs)\n","    y_pred = np.round(y_probs)\n","\n","    # Calculate metrics\n","    accuracy, precision, recall, specificity, roc_auc, fpr, tpr = calculate_metrics(all_labels, y_pred, y_probs)\n","\n","    # Calculate average inference time\n","    average_inference_time = total_inference_time / len(all_videos)\n","\n","    # Print metrics\n","    print(f'Accuracy: {accuracy:.4f}')\n","    print(f'Precision: {precision:.4f}')\n","    print(f'Recall: {recall:.4f}')\n","    print(f'Specificity: {specificity:.4f}')\n","    print(f'ROC AUC: {roc_auc:.4f}')\n","    print(f'Average Inference Time: {average_inference_time:.4f} seconds')\n","\n","    # Print classification report and confusion matrix\n","    print('Classification Report:')\n","    print(classification_report(all_labels, y_pred))\n","\n","    print('Confusion Matrix:')\n","    print(confusion_matrix(all_labels, y_pred))\n","\n","    # Plot ROC curve\n","    plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve for tf_model_mv3_BiLStm_quantizzato_Int_fallback_Float_prunato')\n","\n","if __name__ == '__main__':\n","    inferenceV3_BiLSTM()\n"],"metadata":{"id":"Vc5NhQie9_QF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MobileNet-V3 Small + ConvLSTM (soluzione QPT  FLoat 16 e Pruning)\n","\n","Verranno effettuate in ordine le seguenti operazioni:\n","\n","\n","1.   Caricamento del modello \"prunato\" al 50 %\n","2.   Quantizzazione Float16 del modello prunato.\n","\n","\n","**Attenzione**: Per sfruttare la quantizzazione a FLoat16 è stata utilizzata la Gpu(T4-GPU)\n","\n","I risultati, sono i seguenti:\n","\n","\n","\n","*   Peso modello: circa 6 MB\n","*   Accuratezza modello:  83.98 %\n","*   Auc: circa 97.87 %\n","*   Tempo inferenza medio per batch: 0.3 s\n","*   Consumo Energetico medio per 102 secondi di video: 0.0023  kW/h\n","\n","\n"],"metadata":{"id":"UtsWhqSqnp4X"}},{"cell_type":"code","source":["# Carico il modello prunato(non in formato tflite)\n","from tensorflow.keras.models import load_model\n","\n","\n","pruned_model= load_model('/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/Pruning/50_perc/pruned_model_ConvLSTM.h5')\n","pruned_model.summary()"],"metadata":{"id":"rqir5iAnoGQy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import cv2\n","import os\n","\n","# Dimensioni dei batch di input alla rete neurale\n","batch_size = 16\n","height = 224\n","width = 224\n","channels = 3\n","\n","# Funzione per estrarre frame da un video e creare i batch\n","def generate_video_batches(video_paths, seed=None):\n","    batches = []\n","    for video_path in video_paths:\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame = cv2.resize(frame, (width, height))\n","            frames.append(frame)\n","            if len(frames) == batch_size:\n","                batches.append(frames)\n","                frames = []\n","        cap.release()\n","\n","    for batch in batches:\n","        yield [np.array(batch, dtype=np.float32)]\n","\n","# Imposta il convertitore\n","converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n","\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","\n","# Configurazione della quantizzazione float16\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.target_spec.supported_types = [tf.float16]\n","\n","# Disabilita la conversione delle operazioni tf.TensorList\n","converter._experimental_lower_tensor_list_ops = False\n","\n","tflite_quant_model = converter.convert()\n","\n","with open('model_quantized_float16_pruned_ConvLSTM.tflite', 'wb') as f:\n","    f.write(tflite_quant_model)\n","\n","\n","model_quantized_size = os.path.getsize('model_quantized_float16_pruned_ConvLSTM.tflite')\n","model_quantized_size_mb = model_quantized_size  / (1024*1024)\n","print(f\"Quantizzazione completata e modello salvato come 'model_quantized_float16_pruned_ConvLSTM.tflite'  , Peso modello: {model_quantized_size_mb} MB\")\n"],"metadata":{"id":"Bcj8e7K1n1Ad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funzioni di pre processamento dei video e di inferenza\n","\n","import os\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","import time\n","\n","def load_tflite_model(model_path):\n","    interpreter = tf.lite.Interpreter(model_path=model_path)\n","    interpreter.allocate_tensors()\n","    return interpreter\n","\n","def preprocess_video(video_path, input_shape, batch_size):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, (input_shape[1], input_shape[0]))\n","        frame = frame.astype('float32')\n","        frames.append(frame)\n","\n","    cap.release()\n","    frames = np.array(frames)\n","\n","    if len(frames) >= batch_size:\n","        num_batches = len(frames) // batch_size\n","        frames = frames[:num_batches * batch_size]\n","        batches = np.split(frames, num_batches)\n","    else:\n","        batches = []\n","\n","    return batches\n","\n","def run_inference(interpreter, input_data):\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","    interpreter.set_tensor(input_details[0]['index'], input_data)\n","\n","    # Calcola il tempo di inferenza\n","    start_time = time.time()\n","    interpreter.invoke()\n","    end_time = time.time()\n","    inference_time = end_time - start_time\n","\n","    output_data = interpreter.get_tensor(output_details[0]['index'])\n","\n","    return output_data, inference_time\n","\n","def calculate_metrics(y_true, y_pred, y_probs):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    specificity = recall_score(y_true, y_pred, pos_label=0)\n","    fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n","    roc_auc = auc(fpr, tpr)\n","\n","    return accuracy, precision, recall, specificity, roc_auc, fpr, tpr\n","\n","def plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve'):\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='b', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='r', linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(title)\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","def load_videos_from_folder(folder_path, label, input_shape, batch_size):\n","    videos = []\n","    labels = []\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".mp4\") or filename.endswith(\".avi\"):\n","            video_path = os.path.join(folder_path, filename)\n","            video_batches = preprocess_video(video_path, input_shape, batch_size)\n","            videos.extend(video_batches)\n","            labels.extend([label] * len(video_batches))\n","    return videos, labels\n","\n","\n","from codecarbon import track_emissions\n","@track_emissions(project_name=\"MV3_Small_Inference_ConvLSTM_quantizzato_Float_pruned\")\n","def inferenceV3_ConvLSTM():\n","    model_path = 'model_quantized_float16_pruned_ConvLSTM.tflite'\n","    violence_path = '/content/gdrive/MyDrive/VideoInferenza/Violence'\n","    nonviolence_path = '/content/gdrive/MyDrive/VideoInferenza/NonViolence'\n","    input_shape = (224, 224)\n","    batch_size = 16\n","\n","    interpreter = load_tflite_model(model_path)\n","\n","    violence_videos, violence_labels = load_videos_from_folder(violence_path, 1, input_shape, batch_size)\n","    nonviolence_videos, nonviolence_labels = load_videos_from_folder(nonviolence_path, 0, input_shape, batch_size)\n","\n","    all_videos = violence_videos + nonviolence_videos\n","    all_labels = violence_labels + nonviolence_labels\n","\n","    all_videos = np.array([np.expand_dims(video, axis=0) for video in all_videos])\n","    all_labels = np.array(all_labels)\n","\n","    total_inference_time = 0\n","    y_probs = []\n","    for video in all_videos:\n","        output_data, inference_time = run_inference(interpreter, video)\n","        total_inference_time += inference_time\n","        y_probs.append(output_data.ravel()[0])\n","\n","    y_probs = np.array(y_probs)\n","    y_pred = np.round(y_probs)\n","\n","    # Calculate metrics\n","    accuracy, precision, recall, specificity, roc_auc, fpr, tpr = calculate_metrics(all_labels, y_pred, y_probs)\n","\n","    # Calculate average inference time\n","    average_inference_time = total_inference_time / len(all_videos)\n","\n","    # Print metrics\n","    print(f'Accuracy: {accuracy:.4f}')\n","    print(f'Precision: {precision:.4f}')\n","    print(f'Recall: {recall:.4f}')\n","    print(f'Specificity: {specificity:.4f}')\n","    print(f'ROC AUC: {roc_auc:.4f}')\n","    print(f'Average Inference Time: {average_inference_time:.4f} seconds')\n","\n","    # Print classification report and confusion matrix\n","    print('Classification Report:')\n","    print(classification_report(all_labels, y_pred))\n","\n","    print('Confusion Matrix:')\n","    print(confusion_matrix(all_labels, y_pred))\n","\n","    # Plot ROC curve\n","    plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve for tf_model_mv3_convLStm_quantizzato_Float16_prunato')\n","\n","if __name__ == '__main__':\n","    inferenceV3_ConvLSTM()\n"],"metadata":{"id":"4Pxt11nYodmL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"EuCqbOjCsRE4"}},{"cell_type":"markdown","source":["# MobileNet-V3 Small + BiLSTM (soluzione QPT  FLoat 16 e Pruning)\n","\n","Verranno effettuate in ordine le seguenti operazioni:\n","\n","\n","1.   Caricamento del modello \"prunato\" al 50 %\n","2.   Quantizzazione Float16 del modello prunato.\n","\n","\n","**Attenzione**: Per sfruttare la quantizzazione a FLoat16 è stata utilizzata la Gpu(T4-GPU)\n","\n","I risultati, sono i seguenti:\n","\n","\n","\n","*   Peso modello: circa 57.3 MB\n","*   Accuratezza modello: 69 %\n","*   Auc: circa 97.5 %\n","*   Tempo inferenza medio per batch: 0.28 s\n","*   Consumo Energetico medio per 102 secondi di video: 0.001961  kW/h\n","\n","\n","\n"],"metadata":{"id":"yUq9tJ0usVzM"}},{"cell_type":"code","source":["# Carico il modello prunato (non in formato tflite)\n","from tensorflow.keras.models import load_model\n","\n","\n","pruned_model= load_model('/content/gdrive/MyDrive/Modelli/MobileNet_V3_Small/Pruning/50_perc/pruned_model_BiLSTM.h5')\n","pruned_model.summary()"],"metadata":{"id":"tBdpwtkascgX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import cv2\n","import os\n","\n","# Dimensioni dei batch di input alla rete\n","batch_size = 16\n","height = 224\n","width = 224\n","channels = 3  # RGB\n","\n","# Funzione per estrarre frame da un video e creare i batch\n","def generate_video_batches(video_paths, seed=None):\n","    batches = []\n","    for video_path in video_paths:\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame = cv2.resize(frame, (width, height))\n","            frames.append(frame)\n","            if len(frames) == batch_size:\n","                batches.append(frames)\n","                frames = []\n","        cap.release()\n","\n","    for batch in batches:\n","        yield [np.array(batch, dtype=np.float32)]\n","\n","# Imposta il convertitore\n","converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n","\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","\n","# Configurazione della quantizzazione float16\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.target_spec.supported_types = [tf.float16]\n","\n","converter._experimental_lower_tensor_list_ops = False\n","\n","tflite_quant_model = converter.convert()\n","\n","with open('model_quantized_float16_pruned_BiLSTM.tflite', 'wb') as f:\n","    f.write(tflite_quant_model)\n","\n","\n","model_quantized_size = os.path.getsize('model_quantized_float16_pruned_BiLSTM.tflite')\n","model_quantized_size_mb = model_quantized_size  / (1024*1024)\n","print(f\"Quantizzazione completata e modello salvato come 'model_quantized_float16_pruned_BiLSTM.tflite'  , Peso modello: {model_quantized_size_mb} MB\")\n"],"metadata":{"id":"cBzbFPigsfni"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funzioni di pre processamento dei video e di inferenza\n","\n","import os\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","import time\n","\n","def load_tflite_model(model_path):\n","    interpreter = tf.lite.Interpreter(model_path=model_path)\n","    interpreter.allocate_tensors()\n","    return interpreter\n","\n","def preprocess_video(video_path, input_shape, batch_size):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, (input_shape[1], input_shape[0]))\n","        frame = frame.astype('float32')\n","        frames.append(frame)\n","\n","    cap.release()\n","    frames = np.array(frames)\n","\n","    if len(frames) >= batch_size:\n","        num_batches = len(frames) // batch_size\n","        frames = frames[:num_batches * batch_size]\n","        batches = np.split(frames, num_batches)\n","    else:\n","        batches = []\n","\n","    return batches\n","\n","def run_inference(interpreter, input_data):\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","    interpreter.set_tensor(input_details[0]['index'], input_data)\n","\n","    # Calcola il tempo di inferenza\n","    start_time = time.time()\n","    interpreter.invoke()\n","    end_time = time.time()\n","    inference_time = end_time - start_time\n","\n","    output_data = interpreter.get_tensor(output_details[0]['index'])\n","\n","    return output_data, inference_time\n","\n","def calculate_metrics(y_true, y_pred, y_probs):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    specificity = recall_score(y_true, y_pred, pos_label=0)\n","    fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n","    roc_auc = auc(fpr, tpr)\n","\n","    return accuracy, precision, recall, specificity, roc_auc, fpr, tpr\n","\n","def plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve'):\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='b', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='r', linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(title)\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","def load_videos_from_folder(folder_path, label, input_shape, batch_size):\n","    videos = []\n","    labels = []\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".mp4\") or filename.endswith(\".avi\"):\n","            video_path = os.path.join(folder_path, filename)\n","            video_batches = preprocess_video(video_path, input_shape, batch_size)\n","            videos.extend(video_batches)\n","            labels.extend([label] * len(video_batches))\n","    return videos, labels\n","\n","\n","from codecarbon import track_emissions\n","@track_emissions(project_name=\"MV3_Small_Inference_BiLSTM_quantizzato_Float_pruned\")\n","def inferenceV3_BiLSTM():\n","    model_path = 'model_quantized_float16_pruned_BiLSTM.tflite'\n","    violence_path = '/content/gdrive/MyDrive/VideoInferenza/Violence'\n","    nonviolence_path = '/content/gdrive/MyDrive/VideoInferenza/NonViolence'\n","    input_shape = (224, 224)\n","    batch_size = 16\n","\n","    interpreter = load_tflite_model(model_path)\n","\n","    violence_videos, violence_labels = load_videos_from_folder(violence_path, 1, input_shape, batch_size)\n","    nonviolence_videos, nonviolence_labels = load_videos_from_folder(nonviolence_path, 0, input_shape, batch_size)\n","\n","    all_videos = violence_videos + nonviolence_videos\n","    all_labels = violence_labels + nonviolence_labels\n","\n","    all_videos = np.array([np.expand_dims(video, axis=0) for video in all_videos])\n","    all_labels = np.array(all_labels)\n","\n","    total_inference_time = 0\n","    y_probs = []\n","    for video in all_videos:\n","        output_data, inference_time = run_inference(interpreter, video)\n","        total_inference_time += inference_time\n","        y_probs.append(output_data.ravel()[0])\n","\n","    y_probs = np.array(y_probs)\n","    y_pred = np.round(y_probs)\n","\n","    # Calculate metrics\n","    accuracy, precision, recall, specificity, roc_auc, fpr, tpr = calculate_metrics(all_labels, y_pred, y_probs)\n","\n","    # Calculate average inference time\n","    average_inference_time = total_inference_time / len(all_videos)\n","\n","    # Print metrics\n","    print(f'Accuracy: {accuracy:.4f}')\n","    print(f'Precision: {precision:.4f}')\n","    print(f'Recall: {recall:.4f}')\n","    print(f'Specificity: {specificity:.4f}')\n","    print(f'ROC AUC: {roc_auc:.4f}')\n","    print(f'Average Inference Time: {average_inference_time:.4f} seconds')\n","\n","    # Print classification report and confusion matrix\n","    print('Classification Report:')\n","    print(classification_report(all_labels, y_pred))\n","\n","    print('Confusion Matrix:')\n","    print(confusion_matrix(all_labels, y_pred))\n","\n","    # Plot ROC curve\n","    plot_roc_curve(fpr, tpr, roc_auc, title='ROC Curve for tf_model_mv3_BiLStm_quantizzato_Float16_prunato')\n","\n","if __name__ == '__main__':\n","    inferenceV3_BiLSTM()\n"],"metadata":{"id":"8iH48UvbsjMc"},"execution_count":null,"outputs":[]}]}