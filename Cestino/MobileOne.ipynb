{"cells":[{"cell_type":"markdown","metadata":{"id":"oeGSLl43gNG3"},"source":["# Project Work Hand-on Python for ML: MobileNet e Bi-LSTM per la Violence Detection su dataset \"AIRTLab\"\n","\n","Questo notebook estende gli esperimenti presentati in\n","> P. Sernani, N. Falcionelli, S. Tomassini, P. Contardo and A. F. Dragoni, \"Deep Learning for Automatic Violence Detection: Tests on the AIRTLab Dataset,\" in IEEE Access, vol. 9, pp. 160580-160595, 2021, doi: 10.1109/ACCESS.2021.3131315.\n","\n","Il paper è open access e disponibile qui: [https://ieeexplore.ieee.org/document/9627980](https://ieeexplore.ieee.org/document/9627980).\n","\n","In particolare, l'esperimento testa una **Convolutional Neural Network (CNN) 2D pre-addestrata su Imagenet**, più leggera rispetto a quelli presentati nel paper:\n","\n","- MobileNetV2 ([https://keras.io/api/applications/mobilenet/#mobilenetv2-function](https://keras.io/api/applications/mobilenet/#mobilenetv2-function))\n","\n","In questo notebook, MobileNetV2 viene combinata con una **Bi-LSTM** e dei **layer fully connected** per la classificazione di sequenze di frame dei video del **dataset \"AIRTLab\"** in violente e non-violente.\n","\n","Il dataset è open access e disponibile al seguente link:\n","> <https://github.com/airtlab/A-Dataset-for-Automatic-Violence-Detection-in-Videos>"]},{"cell_type":"markdown","metadata":{"id":"LVerPgcditUj"},"source":["## 1 Operazioni preliminari e Data Pre-Processing (DPP)\n","Le celle seguenti:\n","- **clonano il dataset AIRTLab** nella directory /datarepo;\n","- definiscono le funzioni per il **DPP**, effettuando il **ridimensionamento** dei frame dei video a 224 x 224 (formato usato per l'input di MobileNetV2) e **dividendo i video** in sottosequenze di 16 frame."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s0RwZbk2Je5g"},"outputs":[],"source":["# Scarica il dataset AIRTLab pensato per testare tecniche di Violence Detection\n","# !mkdir /datarepo\n","# !git clone https://github.com/airtlab/A-Dataset-for-Automatic-Violence-Detection-in-Videos.git /datarepo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7338,"status":"ok","timestamp":1717606258811,"user":{"displayName":"AIRT Lab","userId":"06885070839980224766"},"user_tz":-120},"id":"tCVXQsZve1P3","outputId":"03cda9e7-62e1-45d6-d089-da7f7e6f8b55"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["# Monto drive Google\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["!git clone https://github.com/apple/ml-mobileone.git\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LxdcIOxHQxDh","executionInfo":{"status":"ok","timestamp":1717605454421,"user_tz":-120,"elapsed":2551,"user":{"displayName":"AIRT Lab","userId":"06885070839980224766"}},"outputId":"e947fc21-b8b1-4b17-f542-e42f63f183cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'ml-mobileone'...\n","remote: Enumerating objects: 56, done.\u001b[K\n","remote: Counting objects: 100% (2/2), done.\u001b[K\n","remote: Compressing objects: 100% (2/2), done.\u001b[K\n","remote: Total 56 (delta 0), reused 0 (delta 0), pack-reused 54\u001b[K\n","Receiving objects: 100% (56/56), 749.78 KiB | 2.21 MiB/s, done.\n","Resolving deltas: 100% (2/2), done.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9TV2Z605_56E"},"outputs":[],"source":["import torch\n","import sys\n","\n","sys.path.append('/content/ml-mobileone')\n"]},{"cell_type":"code","source":["from mobileone import mobileone\n","\n","model = mobileone(variant='s0', inference_mode=True)\n","checkpoint = torch.load('/content/gdrive/MyDrive/checkpoint.pth.tar')\n","model.load_state_dict(checkpoint)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rg-3FASsUMFo","executionInfo":{"status":"ok","timestamp":1717607026027,"user_tz":-120,"elapsed":419,"user":{"displayName":"AIRT Lab","userId":"06885070839980224766"}},"outputId":"0e2e9025-c3b2-431a-fe57-cfc8cb8f0b74"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["import tensorflow as tf\n","import torch\n","from mobileone import mobileone\n","\n","class PyTorchWrapper(tf.keras.layers.Layer):\n","    def __init__(self, variant='s0', **kwargs):\n","        super(PyTorchWrapper, self).__init__(**kwargs)\n","        self.model = mobileone(variant=variant, inference_mode=True)\n","        self.model.eval()  # Assicurati che il modello sia in modalità valutazione\n","\n","    def call(self, inputs):\n","        # Converte i tensori di input in tensori PyTorch\n","        inputs = torch.tensor(inputs.numpy())\n","        # Passa gli input attraverso il modello PyTorch\n","        outputs = self.model(inputs)\n","        # Converte i tensori di output da PyTorch a TensorFlow\n","        outputs = tf.convert_to_tensor(outputs.detach().numpy())\n","        return outputs\n","\n","    def compute_output_shape(self, input_shape):\n","        # La forma di output è la stessa della forma di input\n","        return input_shape\n","\n","# Esempio di utilizzo del wrapper PyTorch con TimeDistributed\n","input_shape = (None, 10, 32)  # Ad esempio, input shape (batch_size, time_steps, features)\n","model = PyTorchWrapper(variant='s0')"],"metadata":{"id":"_-FGpbycBKVA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6fUOjeTFe1_I"},"outputs":[],"source":["# Sistemazione del Dataset AirtLab\n","# import os\n","# import shutil\n","\n","# def move_files(dataset_path):\n","#     # Definisci i percorsi delle cartelle \"Violence\" e \"NonViolence\"\n","#     violence_path = os.path.join(dataset_path, \"Violence\")\n","#     nonviolence_path = os.path.join(dataset_path, \"NonViolence\")\n","\n","#     # Funzione per spostare i file da cam2 alla cartella superiore\n","#     def move_from_cam2(folder_path):\n","#         cam2_path = os.path.join(folder_path, \"cam2\")\n","#         if os.path.exists(cam2_path) and os.path.isdir(cam2_path):\n","#             for filename in os.listdir(cam2_path):\n","#                 source_path = os.path.join(cam2_path, filename)\n","#                 if os.path.isfile(source_path):\n","#                     dest_path = os.path.join(folder_path, filename)\n","#                     # Controlla se il file esiste già nella cartella superiore\n","#                     if os.path.exists(dest_path):\n","#                         # Aggiungi il suffisso \"-1\" al nome del file\n","#                         base, ext = os.path.splitext(filename)\n","#                         dest_path = os.path.join(folder_path, base + \"-1\" + ext)\n","#                     # Sposta il file\n","#                     shutil.move(source_path, dest_path)\n","#             # Rimuove la cartella cam2 se è vuota\n","#             if not os.listdir(cam2_path):\n","#                 os.rmdir(cam2_path)\n","\n","#     # Sposta i file da cam2 per entrambe le cartelle\n","#     move_from_cam2(violence_path)\n","#     move_from_cam2(nonviolence_path)\n","\n","\n","# # Percorso alla cartella principale del dataset\n","# dataset_path = '/content/gdrive/My Drive/Dataset/AirtLab-Dataset'\n","# move_files(dataset_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8gLhBYWKjmxL"},"outputs":[],"source":["# # Unione dei due Dataset disponibili, Kaggle e AirtLab\n","\n","# def merge_datasets(src1, src2, dst):\n","#     # Creazione delle cartelle di destinazione \"Violence\" e \"NonViolence\"\n","#     os.makedirs(os.path.join(dst, \"Violence\"), exist_ok=True)\n","#     os.makedirs(os.path.join(dst, \"NonViolence\"), exist_ok=True)\n","\n","#     # Funzione per unire i contenuti delle cartelle sorgente nella cartella di destinazione\n","#     def merge_category(src_path, dst_path):\n","#         for filename in os.listdir(src_path):\n","#             src_file_path = os.path.join(src_path, filename)\n","#             if os.path.isfile(src_file_path):\n","#                 dest_file_path = os.path.join(dst_path, filename)\n","#                 # Controlla se il file esiste già nella cartella di destinazione\n","#                 if os.path.exists(dest_file_path):\n","#                     # Aggiungi il suffisso \"-1\" al nome del file\n","#                     base, ext = os.path.splitext(filename)\n","#                     dest_file_path = os.path.join(dst_path, base + \"-1\" + ext)\n","#                 # Copia il file nella cartella di destinazione\n","#                 shutil.copy2(src_file_path, dest_file_path)\n","\n","#     # Unisci i contenuti delle cartelle \"Violence\"\n","#     merge_category(os.path.join(src1, \"Violence\"), os.path.join(dst, \"Violence\"))\n","#     merge_category(os.path.join(src2, \"Violence\"), os.path.join(dst, \"Violence\"))\n","\n","#     # Unisci i contenuti delle cartelle \"NonViolence\"\n","#     merge_category(os.path.join(src1, \"NonViolence\"), os.path.join(dst, \"NonViolence\"))\n","#     merge_category(os.path.join(src2, \"NonViolence\"), os.path.join(dst, \"NonViolence\"))\n","\n","# # Percorsi alle cartelle sorgenti e alla cartella di destinazione\n","# src1 = '/content/gdrive/My Drive/Dataset/KaggleViolenceNonViolence'\n","# src2 = '/content/gdrive/My Drive/Dataset/AirtLab-Dataset'\n","# dst = '/content/gdrive/My Drive/Dataset/Dataset-Edge'\n","\n","# # Unisci i dataset\n","# merge_datasets(src1, src2, dst)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2_-Hx-iyKeNG"},"outputs":[],"source":["# Funzioni per DPP e utilità (chunk count, video preprocessing, feature computation, ...)\n","import os\n","import cv2\n","import numpy as np\n","\n","def count_chunks(videoBasePath):\n","    \"\"\"Counts the 16 frames lenght chunks available in a dataset organized in violent and non-violent,\n","    cam1 and cam2 folders, placed at videoBasePath.\n","\n","    Parameters\n","    ----------\n","    videoBasePath : str\n","                    Base path of the dataset\n","\n","    Returns\n","    -------\n","    cnt : int\n","          number of 16 frames lenght chunks in the dataset\n","    \"\"\"\n","\n","    folders = ['Violence', 'NonViolence']\n","    cams = ['cam1', 'cam2']\n","    cnt = 0\n","\n","    for folder in folders:\n","        for camName in cams:\n","            path = os.path.join(videoBasePath, folder, camName)\n","\n","            videofiles = os.listdir(path)\n","            for videofile in videofiles:\n","                filePath = os.path.join(path, videofile)\n","                video = cv2.VideoCapture(filePath)\n","                numframes = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n","                fps = int(video.get(cv2.CAP_PROP_FPS))\n","                chunks = numframes//16\n","                cnt += chunks\n","\n","\n","    return cnt\n","\n","def preprocessVideos(videoBasePath, mainDir, featureBasePath, verbose=True):\n","    \"\"\"Preproccess all the videos.\n","\n","    It extracts samples from the videos organised in violent and non-violent, cam1 and cam2 folders.\n","    The samples and the labels are store on two memmap numpy arrays, called samples.mmap and labels.mmap, at \"featureBasePath\".\n","    The numpy array with samples has shape (Chunk #, 16, 224, 224, 3), the labels array has shape (Chunk # 16, 224, 224, 3).\n","    For the AIRTLab dataset the number of chunks is 3537.\n","\n","    Parameters\n","    ----------\n","    videoBasePath : str\n","                    Pathname to the base of the video repository, which contains two directories,\n","                    violent and non-violent, which are divided into cam1 and cam2.\n","    mainDir: str\n","             Pathaname to store the files with sample filenames and labels.\n","    featureBasePath : str\n","                      it is the pathname of a base where the numpy arrays have to be saved.\n","    verbose : bool\n","              if True print debug logs (default True)\n","\n","    \"\"\"\n","\n","    folders = ['Violence', 'NonViolence']\n","    cams = ['cam1', 'cam2']\n","    total_chunks = count_chunks(videoBasePath)\n","    fileNames = []\n","    npLabels = np.zeros(total_chunks)\n","    cnt = 0\n","\n","    for folder in folders:\n","        for camName in cams:\n","            path = os.path.join(videoBasePath, folder, camName)\n","\n","            videofiles = os.listdir(path)\n","            for videofile in videofiles:\n","                filePath = os.path.join(path, videofile)\n","                video = cv2.VideoCapture(filePath)\n","                numframes = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n","                fps = int(video.get(cv2.CAP_PROP_FPS))\n","                chunks = numframes//16\n","                if verbose:\n","                    print(filePath)\n","                    print(\"*** [Video Info] Number of frames: {} - fps: {} - chunks: {}\".format(numframes, fps, chunks))\n","                vid = []\n","                videoFrames = []\n","                while True:\n","                    ret, img = video.read()\n","                    if not ret:\n","                        break\n","                    videoFrames.append(cv2.resize(img, (224, 224)))\n","                vid = np.array(videoFrames, dtype=np.float32)\n","                filename = os.path.splitext(videofile)[0]\n","                chunk_cnt = 0\n","                for i in range(chunks):\n","                    X = vid[i*16:i*16+16]\n","                    chunk_cnt += 1\n","                    filename = folder + '_' + camName + '_' + videofile + '_chunk_' + str(chunk_cnt) + '.npy'\n","                    fileNames.append(filename)\n","                    savepath = os.path.join(featureBasePath, filename)\n","                    np.save(savepath, np.array(X, dtype=np.float32))\n","                    if folder == 'Violence':\n","                        npLabels[cnt] = np.int8(1)\n","                    else:\n","                        npLabels[cnt] = np.int8(0)\n","                    cnt += 1\n","\n","    fileNamesNp = os.path.join(mainDir, 'filenames.npy')\n","    np.save(fileNamesNp, fileNames)\n","\n","    labelsNp = os.path.join(mainDir, 'labels.npy')\n","    np.save(labelsNp, npLabels)\n","\n","    if verbose:\n","        print(\"** Labels **\")\n","        print(npLabels.shape)\n","        print('\\n****\\n')\n","        print(\"** Samples **\")\n","        print(len(fileNames))\n","        print('\\n****\\n')\n","\n","    del fileNames\n","    del npLabels"]},{"cell_type":"markdown","metadata":{"id":"33T200nDn2sq"},"source":["## 2 Esecuzione Data Pre-Processing (DPP)\n","Le celle seguenti:\n","- creano le cartelle dove memorizzare i vettori di feature estratte dalle sottosequenze;\n","- eseguono il DPP su tutti i video del dataset, trasformandoli in sottosequenze di **16 frame** a **224 x 224**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5tK07PDR6OC"},"outputs":[],"source":["# Cartelle per memorizzare i campioni (le sottosequenze di 16 frame) da usare\n","# per training, validation e test.\n","#!rm -rf airtlabDataset\n","#!mkdir airtlabDataset\n","#!mkdir airtlabDataset/features\n","#!mkdir airtlabDataset/results\n","\n","\n","paths = [\"/airtlabDataset\", \"/airtlabDataset/features\", \"/airtlabDataset/results\"]\n","for path in paths:\n","  if not os.path.isdir(path):\n","    os.mkdir(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8YrDkTN2Kdj"},"outputs":[],"source":["preprocessVideos('/content/gdrive/My Drive/Dataset/AirtLab-Dataset', '/airtlabDataset', '/airtlabDataset/features', True)"]},{"cell_type":"markdown","metadata":{"id":"A1i9eyYf53DF"},"source":["## 3 Esperimento\n","\n","The following cells\n","- define some utilities functions to build the end-to-end models composed of 2D CNNs and Bi-LSTM and to run the experiments on such models; the experiments are tests repeated **5 times** with the **stratified shuffle split** cross-validation scheme. In each split 80% of data are used for training, and 20% of data are used for testing. 12,5% of the training data (i.e. 10% of the entire dataset) is used for validation. In other words, in each test **70%** of data are actually for **training**, **10%** for **validation**, and **20%** for **testing**.\n","- run the experiments model by model.\n","\n","Each tested model is composed according to the following table. Note that the weights of the **2D CNNs are the ImageNet weights**, whereas **the other layers are trained on the AIRTLab dataset**.\n","\n","| Layer Type                                     | Output Shape         | Parameter # |\n","|:-----------------------------------------------|:---------------------|------------:|\n","| Time Distributed 2D CNN                        | -                    |      -            |\n","| Time Distributed Flatten                       | -                   |           0 |\n","| Bi-LSTM, *128 hidden units*                    | (None, 256)          |           - |\n","| Dropout, *0.5*                                 | (None, 256)          |           0 |\n","| Dense, *128 units*, *ReLU activation*          | (None, 128)          |       32896 |\n","| Dropout, *0.5*                                 | (None, 128)          |           0 |\n","| Dense,  *1 unit*, *Sigmoid activation*         | (None, 1)            |         129 |"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rGdFcPb7fYz4"},"outputs":[],"source":["# definitions of two end-to-end models + definitions of experiments\n","import pandas as pd\n","import numpy as np\n","import sklearn\n","from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n","from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, classification_report\n","from keras.callbacks import EarlyStopping\n","import matplotlib.pylab as plt\n","import os\n","from keras.models import Sequential, Model\n","from keras.layers import Input, Dense, Dropout, Flatten, ConvLSTM2D, TimeDistributed, Bidirectional, LSTM\n","#from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input as vgg16_preprocess_input\n","#from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input as mobilenet_v2_preprocess_input\n","from keras.utils import Sequence\n","import torch\n","import torch.nn as nn\n","\n","class DataGen(Sequence):\n","    \"\"\" A sequence of data for training/test/validation, loaded from memory\n","    batch by batch. Extends the tensorflow.keras.utils.Sequence: https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n","\n","    Attributes\n","    ----------\n","    base_path : str\n","                path to the folder including the samples.\n","    filenames : list<str>\n","                list of sample filenames.\n","    labels : list<str>\n","             list of sample labels.\n","    batch_size : int\n","                 batch size to load samples\n","\n","    \"\"\"\n","\n","    def __init__(self, base_path, filenames, labels, batch_size, Preprocess_input):\n","        self.base_path = base_path\n","        self.filenames = filenames\n","        self.labels = labels\n","        self.batch_size = batch_size\n","        self.Preprocess_input = Preprocess_input\n","\n","    def __len__(self):\n","        return (np.ceil(len(self.filenames) / float(self.batch_size))).astype(int)\n","\n","    def __getitem__(self, idx):\n","        batch_x = self.filenames[idx * self.batch_size: (idx + 1) * self.batch_size]\n","        batch_y = self.labels[idx * self.batch_size: (idx + 1) * self.batch_size]\n","\n","        return np.array([self.Preprocess_input(np.load(os.path.join(self.base_path, file_name))) for file_name in batch_x]), np.array(batch_y)\n","\n","def GetPretrainedModel(ModelConstructor, input_shape=(224, 224, 3), print_summary=True):\n","    \"\"\" Builds the VGG16 2D CNN with the Imagenet weights, freezing all layers except layers_to_finetune\n","\n","    Parameters\n","    ----------\n","    ModelConstructor : Callable[[bool], [str], [tuple], Sequential]\n","                       Function that download the pretrained model, i.e. one of the Keras applications:\n","                       https://keras.io/api/applications/\n","                       The arguments are include_top, weights, and input_shape.\n","    input_shape : tuple\n","                  The input shape for the pretrained model.\n","    print_summary : bool\n","                    If True prints the model summary.\n","\n","    Returns\n","    -------\n","    model : Sequential\n","          The instantiated model.\n","    \"\"\"\n","\n","    model = ModelConstructor # (include_top=False, input_shape=input_shape)\n","\n","    for param in model.parameters():\n","      param.requires_grad = False\n","\n","    return model\n","\n","def getLSTMModel(getConvModel, ModelConstructor, pretrained_input_shape=(224, 224, 3), verbose=True):\n","    \"\"\"Creates the BiLSTM + fully connected layers end-to-end model object\n","    with the sequential API: https://keras.io/models/sequential/\n","\n","    Parameters\n","    ----------\n","    getConvModel : Callable[Callable[[bool], [str], [tuple], Sequential], [tuple], [bool], Sequential]\n","                Function that instantiates the pretrained Convolutional model\n","                to be applied in a time distributed fashion.\n","    ModelConstructor : Callable[[bool], [str], [tuple], Sequential]\n","                       Function that download the pretrained model, i.e. one of the Keras applications:\n","                       https://keras.io/api/applications/\n","                       The arguments are include_top, weights, and input_shape.\n","    input_shape : tuple\n","                  The input shape for the pretrained model.\n","    verbose : bool\n","              if True prints the model summary (default True)\n","\n","    Returns\n","    -------\n","    model : Sequential\n","            The instantiated model\n","    \"\"\"\n","    model = Sequential()\n","    model.add(TimeDistributed(getConvModel(ModelConstructor, pretrained_input_shape, verbose), input_shape=(16, 224, 224, 3)))\n","\n","    model.add(TimeDistributed(Flatten()))\n","    model.add(Bidirectional(LSTM(units=128, return_sequences=False)))\n","    #model.add(LSTM(units=128, return_sequences=False))\n","\n","    model.add(Dropout(0.5))\n","    model.add(Dense(128, activation='relu'))\n","\n","    model.add(Dropout(0.5))\n","    model.add(Dense(1, activation='sigmoid'))\n","    if verbose:\n","        model.summary()\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    return model\n","\n","def getConvLSTMModel(getConvModel, ModelConstructor, pretrained_input_shape=(224, 224, 3), verbose=True):\n","    \"\"\"Creates the BiLSTM + fully connected layers end-to-end model object\n","    with the sequential API: https://keras.io/models/sequential/\n","\n","    Parameters\n","    ----------\n","    getConvModel : Callable[Callable[[bool], [str], [tuple], Sequential], [tuple], [bool], Sequential]\n","                Function that instantiates the pretrained Convolutional model\n","                to be applied in a time distributed fashion.\n","    ModelConstructor : Callable[[bool], [str], [tuple], Sequential]\n","                       Function that download the pretrained model, i.e. one of the Keras applications:\n","                       https://keras.io/api/applications/\n","                       The arguments are include_top, weights, and input_shape.\n","    input_shape : tuple\n","                  The input shape for the pretrained model.\n","    verbose : bool\n","              if True prints the model summary (default True)\n","\n","    Returns\n","    -------\n","    model : Sequential\n","            The instantiated model\n","    \"\"\"\n","    model = Sequential()\n","    model.add(TimeDistributed(getConvModel(ModelConstructor, pretrained_input_shape, verbose), input_shape=(16, 224, 224, 3)))\n","\n","    model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3)))\n","\n","    model.add(Flatten())\n","    model.add(Dropout(0.5))\n","    model.add(Dense(256, activation='relu'))\n","\n","    model.add(Dropout(0.5))\n","    model.add(Dense(1, activation='sigmoid'))\n","    if verbose:\n","        model.summary()\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    return model\n","\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\n","from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n","\n","def runEndToEndExperiment(getLSTMModel, getConvModel, ModelConstructor, pretrained_input_shape, Preprocess_input, batchSize, datasetBasePath, npyBasePath, featuresPath, samplesMMapName, lablesMMapName, endToEndModelName, rState, savePath):\n","    \"\"\"Runs the tests with end to end models.\n","\n","    Parameters\n","    ----------\n","    getLSTMModel : Callable[[Callable[Callable[[bool], [str], [tuple], Sequential],\n","                   [tuple], [bool], Sequential]], [Callable[[bool], [str], [tuple],\n","                   Sequential]], [tuple], [bool], Sequential]\n","                   Function that instantiates the model to be tested. The parameters\n","                   are a function that returns the Convolutional model to be tested\n","                   in a time distributed fashion, and a boolean for verbose output\n","    getConvModel : Callable[Callable[[bool], [str], [tuple], Sequential], [tuple], [bool], Sequential]\n","                Function that instantiates the pretrained Convolutional model\n","                to be applied in a time distributed fashion.\n","    ModelConstructor : Callable[[bool], [str], [tuple], Sequential]\n","                       Function that download the pretrained model, i.e. one of the Keras applications:\n","                       https://keras.io/api/applications/\n","                       The arguments are include_top, weights, and input_shape.\n","    input_shape : tuple\n","                  The input shape for the pretrained model.\n","    batchSize : int\n","                Batch size to be used for training and testing\n","    datasetBasePath : str\n","                      Pathname to the base of the feature files repository,\n","                      which contains two directories, violent and non-violent,\n","                      which are divided into cam1 and cam2.\n","    npyBasePath : str\n","                  Pathname where the files with sample filenames and labels are\n","                  stored.\n","    featuresPath : str\n","                  Folder containing the actual files with the samples.\n","    samplesMMapName : str\n","                      Name of the file storing the list with sample filenames.\n","    lablesMMapName : str\n","                     Name of the file storing the list of sample labels.\n","    endToEndModelName : str\n","                        Model name to be used in the AUC-ROC plot.\n","    rState : int, RandomState instance or None\n","             Controls the randomness of the training and testing indices produced.\n","             Pass an int for reproducible output across multiple function calls.\n","    savePath : str\n","               Path to the directory where the model and weights will be saved.\n","    \"\"\"\n","    chunk_number = count_chunks(datasetBasePath)\n","    X = np.load(os.path.join(npyBasePath, samplesMMapName))\n","    y = np.load(os.path.join(npyBasePath, lablesMMapName))\n","\n","    nsplits = 5\n","    cv = StratifiedShuffleSplit(n_splits=nsplits, train_size=0.8, random_state=rState)\n","\n","    tprs = []\n","    aucs = []\n","    scores = []\n","    sens = np.zeros(shape=(nsplits))\n","    specs = np.zeros(shape=(nsplits))\n","    f1Scores = np.zeros(shape=(nsplits))\n","    mean_fpr = np.linspace(0, 1, 100)\n","    plt.figure(num=1, figsize=(10, 10))\n","    i = 1\n","\n","    for train, test in cv.split(X, y):\n","\n","        X_train, X_val, y_train, y_val = train_test_split(X[train][:], y[train], test_size=0.125, random_state=rState)\n","\n","        filepath = os.path.join(npyBasePath, featuresPath)\n","\n","        training_batch_generator = DataGen(filepath, X_train, y_train, batchSize, Preprocess_input)\n","        validation_batch_generator = DataGen(filepath, X_val, y_val, batchSize, Preprocess_input)\n","        test_batch_generator = DataGen(filepath, X[test][:], y[test], batchSize, Preprocess_input)\n","\n","        model = getLSTMModel(getConvModel, ModelConstructor, pretrained_input_shape, i == 1)\n","\n","        # Define the ModelCheckpoint callback to save the best weights\n","        checkpoint_path = os.path.join(savePath, f\"best_model_fold_{i}_epoch_{{epoch:02d}}.h5\")\n","        mc = ModelCheckpoint(checkpoint_path, monitor='val_loss', mode='min', save_best_only=True, save_weights_only=True, verbose=1)\n","\n","        es = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1, restore_best_weights=True)\n","\n","        model.fit(x=training_batch_generator, validation_data=validation_batch_generator, epochs=50, verbose=1, callbacks=[es, mc])\n","\n","        del X_train\n","        del X_val\n","\n","        print(\"Computing scores...\")\n","        evaluation = model.evaluate(x=test_batch_generator)\n","        scores.append(evaluation)\n","        print(\"Computing probs...\")\n","        probas = model.predict(x=test_batch_generator, verbose=1).ravel()\n","\n","        fpr, tpr, thresholds = roc_curve(y[test], probas)\n","        tprs.append(np.interp(mean_fpr, fpr, tpr))\n","        roc_auc = auc(fpr, tpr)\n","        aucs.append(roc_auc)\n","        plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC split %d (AUC = %0.4f)' % (i, roc_auc))\n","\n","        y_pred = np.round(probas)\n","        report = classification_report(y[test], y_pred, target_names=['non-violent', 'violent'], output_dict=True)\n","        sens[i - 1] = report['violent']['recall']\n","        specs[i - 1] = report['non-violent']['recall']\n","        f1Scores[i - 1] = report['violent']['f1-score']\n","\n","        print('confusion matrix split ' + str(i))\n","        print(confusion_matrix(y[test], y_pred))\n","        print(classification_report(y[test], y_pred, target_names=['non-violent', 'violent']))\n","        print('Loss: ' + str(evaluation[0]))\n","        print('Accuracy: ' + str(evaluation[1]))\n","        print('\\n')\n","\n","        # Save the final model of the current fold\n","        final_model_path = os.path.join(savePath, f'final_model_fold_{i}.h5')\n","        model.save(final_model_path)\n","\n","        i += 1\n","\n","        del report\n","        del model\n","\n","    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n","\n","    mean_tpr = np.mean(tprs, axis=0)\n","    mean_auc = auc(mean_fpr, mean_tpr)\n","    std_auc = np.std(aucs)\n","    plt.plot(mean_fpr, mean_tpr, color='b', label=r'Mean ROC (AUC = %0.4f $\\pm$ %0.4f)' % (mean_auc, std_auc), lw=2, alpha=.8)\n","\n","    std_tpr = np.std(tprs, axis=0)\n","    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n","    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n","    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label=r'$\\pm$ 1 std. dev.')\n","\n","    plt.xlim([-0.01, 1.01])\n","    plt.ylim([-0.01, 1.01])\n","    plt.xlabel('False Positive Rate', fontsize=18)\n","    plt.ylabel('True Positive Rate', fontsize=18)\n","    plt.title('Cross-Validation ROC of ' + endToEndModelName + ' model', fontsize=18)\n","    plt.legend(loc=\"lower right\", prop={'size': 15})\n","\n","    np_scores = np.array(scores)\n","    losses = np_scores[:, 0:1]\n","    accuracies = np_scores[:, 1:2]\n","    print('Losses')\n","    print(losses)\n","    print('Accuracies')\n","    print(accuracies)\n","    print('Sensitivities')\n","    print(sens)\n","    print('Specificities')\n","    print(specs)\n","    print('F1-scores')\n","    print(f1Scores)\n","    print(\"Avg loss: {0} +/- {1}\".format(np.mean(losses), np.std(losses)))\n","    print(\"Avg accuracy: {0} +/- {1}\".format(np.mean(accuracies), np.std(accuracies)))\n","    print(\"Avg sensitivity: {0} +/- {1}\".format(np.mean(sens), np.std(sens)))\n","    print(\"Avg specificity: {0} +/- {1}\".format(np.mean(specs), np.std(specs)))\n","    print(\"Avg f1-score: {0} +/- {1}\".format(np.mean(f1Scores), np.std(f1Scores)))\n","\n","    plt.savefig(endToEndModelName.replace('+', '') + '.pdf')\n","    plt.show()\n"]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import TimeDistributed\n","from mobileone_tf import MobileOne\n"],"metadata":{"id":"2wC2Qyf3ah8L"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YpcrpnbeS9xJ"},"outputs":[],"source":["from tensorflow.keras.applications.mobilenet import preprocess_input as mobilenet_v1_preprocess_input\n","\n","runEndToEndExperiment(\n","    getLSTMModel,\n","    GetPretrainedModel,\n","    model,\n","    (224, 224, 3),\n","    mobilenet_v1_preprocess_input,\n","    8,\n","    '/content/gdrive/My Drive/Dataset/AirtLab-Dataset',\n","    '/airtlabDataset',\n","    'features',\n","    'filenames.npy',\n","    'labels.npy',\n","    'MobileNetV3Small + BiLSTM',\n","    42,\n","    '/content/gdrive/ My Drive/MobileOneS0_BiLSTM'\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJeCupPlcHtE"},"outputs":[],"source":["from tensorflow.keras.applications.mobilenet import preprocess_input as mobilenet_v1_preprocess_input\n","\n","\n","runEndToEndExperiment(\n","    getConvLSTMModel,\n","    GetPretrainedModel,\n","    model,\n","    (224, 224, 3),\n","    mobilenet_v1_preprocess_input,\n","    8,\n","    '/content/gdrive/My Drive/Dataset/AirtLab-Dataset',\n","    '/airtlabDataset',\n","    'features',\n","    'filenames.npy',\n","    'labels.npy',\n","    'MobileNetV3Small + ConvLSTM',\n","    42,\n","    '/content/gdrive/ My Drive/MobileOneS0_ConvLSTM'\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFnAGLn85_WM"},"outputs":[],"source":["from keras import __version__\n","from keras import backend as K\n","import sklearn\n","\n","print('Using Keras version:', __version__, 'backend:', K.backend())\n","\n","if K.backend() == \"tensorflow\":\n","    import tensorflow as tf\n","    device_name = tf.test.gpu_device_name()\n","    if device_name == '':\n","        device_name = \"None\"\n","    print('Using TensorFlow version:', tf.__version__, ', GPU:', device_name)\n","\n","print('The scikit-learn version is {}.'.format(sklearn.__version__))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"alLhTYiB8TQh"},"outputs":[],"source":["!nvidia-smi"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}